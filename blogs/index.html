<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <!-- Meta tags dynamically injected by <Seo /> component via react-helmet-async -->
    <!-- Static fallbacks only for crawlers that don't execute JS -->
    <title>Blog | Imadlab | Research Engineer &amp; Internal CTO</title>
    <meta name="description" content="Technical notes on industrial AI systems, local LLM extraction pipelines, architecture, and evaluation." />
    <!-- prerender-seo:start -->
    <link rel="canonical" href="https://imadlab.me/blogs" />
    <meta property="og:title" content="Blog | Imadlab | Research Engineer &amp; Internal CTO" />
    <meta property="og:description" content="Technical notes on industrial AI systems, local LLM extraction pipelines, architecture, and evaluation." />
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://imadlab.me/blogs" />
    <meta property="og:image" content="https://imadlab.me/images/hero-moon.png" />
    <meta property="og:image:alt" content="Blog | Imadlab | Research Engineer &amp; Internal CTO" />
    <meta property="og:locale" content="en_US" />
    <meta property="og:site_name" content="Imadlab" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:site" content="@imadlab" />
    <meta name="twitter:creator" content="@imadlab" />
    <meta name="twitter:title" content="Blog | Imadlab | Research Engineer &amp; Internal CTO" />
    <meta name="twitter:description" content="Technical notes on industrial AI systems, local LLM extraction pipelines, architecture, and evaluation." />
    <meta name="twitter:image" content="https://imadlab.me/images/hero-moon.png" />
    <meta name="twitter:image:alt" content="Blog | Imadlab | Research Engineer &amp; Internal CTO" />
    <script type="application/ld+json">{"@context":"https://schema.org","@type":"CollectionPage","name":"Blog","url":"https://imadlab.me/blogs"}</script>
    <!-- prerender-seo:end -->

    <!-- Favicon and Theme Color -->
    <link rel="icon" href="/favicon.ico" sizes="any">
    <link rel="apple-touch-icon" href="/apple-touch-icon.png">
    <meta name="theme-color" content="#000000" />

    <!-- Preload hero image for faster LCP on home route -->
    <link rel="preload" as="image" href="/images/hero-moon.avif" type="image/avif" />

    <!-- Font Preconnect for Performance -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    
    <!-- Load fonts with font-display: swap for better performance -->
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800;900&family=JetBrains+Mono:wght@300;400;500;600;700&display=swap" rel="stylesheet" media="print" onload="this.media='all'">
    <noscript>
      <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800;900&family=JetBrains+Mono:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    </noscript>
    
    <!-- DNS prefetch for Supabase and Cloudflare Analytics -->
    <link rel="dns-prefetch" href="https://mpkgugcasxpanhrkpkhs.supabase.co">
    <link rel="preconnect" href="https://mpkgugcasxpanhrkpkhs.supabase.co" crossorigin>
    <link rel="dns-prefetch" href="https://static.cloudflareinsights.com">
    <link rel="preconnect" href="https://static.cloudflareinsights.com" crossorigin>
    
    <!-- RSS/JSON Feeds -->
    <link rel="alternate" type="application/rss+xml" title="Imadlab Blog RSS Feed" href="/feed.xml" />
    <link rel="alternate" type="application/feed+json" title="Imadlab Blog JSON Feed" href="/feed.json" />
    <meta name="referrer" content="strict-origin-when-cross-origin" />

    <!-- Structured Data -->
    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "WebSite",
      "url": "https://imadlab.me",
      "name": "Imadlab",
      "description": "Applied research and engineering work by Imad Eddine El Mouss on multimodal industrial AI, procedural knowledge extraction, and deployable systems.",
      "publisher": {
        "@type": "Person",
        "name": "Imad Eddine El Mouss",
        "url": "https://imadlab.me/about",
        "sameAs": [
          "https://github.com/imaddde867",
          "https://www.linkedin.com/in/imad-eddine-e-986741262"
        ]
      },
      "potentialAction": {
        "@type": "SearchAction",
        "target": "https://duckduckgo.com/?q=site%3Aimadlab.me+{search_term_string}",
        "query-input": "required name=search_term_string"
      }
    }
    </script>
    <script type="module" crossorigin src="/assets/js/index-B1IGmRtD.js"></script>
    <link rel="stylesheet" crossorigin href="/assets/index-9eB0Q-OK.css">
  </head>

  <body>
    <h1 style="position: absolute; width: 1px; height: 1px; padding: 0; margin: -1px; overflow: hidden; clip: rect(0, 0, 0, 0); border: 0;">Imadlab | Research Engineer & Internal CTO</h1>
    <div id="root">
<main data-prerender="true" class="prerender-shell">
  <h1 class="text-3xl font-bold mb-4">Blog</h1>
  <section class="prerender-grid">

    <article class="prerender-card">
      <div class="prerender-image"><img src="https://miro.medium.com/1*d3PZ7JqYIy_ENB2zuUOk3Q.jpeg" alt="Evaluation Beyond Accuracy: Constraint Coverage and Safety" loading="lazy" decoding="async" /></div>
      <h2 class="prerender-title">Evaluation Beyond Accuracy: Constraint Coverage and Safety</h2>
      <p class="prerender-meta">Published February 4, 2026</p>
      <p class="prerender-tags">Tags: evaluation, industrial-ai, safety, procedural-knowledge-extraction, reliability</p>
      <p class="prerender-summary">Why standard NLP accuracy numbers are not enough for industrial procedure extraction, and what to measure instead.</p>
      <a href="/blogs/evaluation-beyond-accuracy-constraint-coverage-and-safety" class="prerender-link">Read article</a>
    </article>

    <article class="prerender-card">
      <div class="prerender-image"><img src="https://substackcdn.com/image/fetch/$s_!mMj9!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1dd3c4b8-031a-469f-a0c1-6c052b5d3f58_2562x1231.png" alt="Designing Local LLM Pipelines for Industrial Documentation" loading="lazy" decoding="async" /></div>
      <h2 class="prerender-title">Designing Local LLM Pipelines for Industrial Documentation</h2>
      <p class="prerender-meta">Published January 16, 2026</p>
      <p class="prerender-tags">Tags: local-llm, industrial-ai, privacy-by-design, systems-engineering, architecture</p>
      <p class="prerender-summary">A field guide for building local document intelligence pipelines when privacy, latency, and traceability are non-negotiable constraints.</p>
      <a href="/blogs/designing-local-llm-pipelines-for-industrial-documentation" class="prerender-link">Read article</a>
    </article>

    <article class="prerender-card">
      <div class="prerender-image"><img src="https://miro.medium.com/v2/resize:fit:2000/format:webp/1*jNWQdkJwTMSN4P9_OvqBsQ.png" alt="Procedural Knowledge Extraction Is Not Summarization" loading="lazy" decoding="async" /></div>
      <h2 class="prerender-title">Procedural Knowledge Extraction Is Not Summarization</h2>
      <p class="prerender-meta">Published December 29, 2025</p>
      <p class="prerender-tags">Tags: procedural-knowledge-extraction, industrial-ai, knowledge-graphs, evaluation, safety</p>
      <p class="prerender-summary">In industrial documentation, a fluent summary can still be operationally wrong. This note explains why procedural extraction needs structure, constraints, and traceable validation.</p>
      <a href="/blogs/procedural-knowledge-extraction-is-not-summarization" class="prerender-link">Read article</a>
    </article>

    <article class="prerender-card">
      <div class="prerender-image"><img src="https://images.manning.com/book/1/92362be-2647-4fa1-9998-e4959d9c568f/DOTD_Raschka.png" alt="Designing a Small Local LLM Pipeline on Apple Silicon: Engineering Notes" loading="lazy" decoding="async" /></div>
      <h2 class="prerender-title">Designing a Small Local LLM Pipeline on Apple Silicon: Engineering Notes</h2>
      <p class="prerender-meta">Published July 11, 2025</p>
      <p class="prerender-tags">Tags: local-llm, industrial-ai, evaluation, privacy-by-design, systems-engineering</p>
      <p class="prerender-summary">A practical engineering note on building and evaluating a local transformer pipeline under tight hardware constraints, including failure modes and reliability tradeoffs.</p>
      <a href="/blogs/local-llm-apple-silicon-engineering-notes" class="prerender-link">Read article</a>
    </article>
  </section>
</main>
</div>
    
  <script>window.__PRERENDERED_DATA__ = window.__PRERENDERED_DATA__ || {}; window.__PRERENDERED_DATA__["posts"] = [{"id":"8999aba3-c346-4e57-9ce9-cba1dc6459a0","title":"Evaluation Beyond Accuracy: Constraint Coverage and Safety","slug":"evaluation-beyond-accuracy-constraint-coverage-and-safety","excerpt":"Why standard NLP accuracy numbers are not enough for industrial procedure extraction, and what to measure instead.","body":"Most extraction systems look acceptable if you only report one headline number.\n\nI have made that reporting mistake myself, and it hides the real risk.\n\nIn industrial procedure support, a model can score well on overlap-style metrics and still fail where risk is concentrated: conditions, prohibitions, and exception logic. If evaluation ignores that gap, teams get a false sense of readiness.\n\n## The limits of standard \"accuracy\"\n\nFor procedural tasks, classic metrics can over-reward the easy parts:\n\n- action verbs,\n- frequent entities,\n- common step patterns.\n\nThe hard parts are less frequent but more important:\n\n- threshold conditions,\n- decision branches,\n- exception paths,\n- role-specific approvals.\n\nIf a system misses one high-risk guard and captures twenty low-risk steps, aggregate accuracy can still look strong. Operationally, that is still a failing system.\n\n## A metric suite aligned with risk\n\nI use a metric set that separates structural correctness from safety-critical fidelity.\n\n| Metric | What it checks | Why it matters |\n| --- | --- | --- |\n| Step F1 | Core step extraction quality | Baseline structural coverage |\n| Adjacency fidelity | Correct step ordering and transitions | Process logic continuity |\n| Constraint coverage | Recovery of required conditions and guards | Safety-critical completeness |\n| Decision-point fidelity | Preservation of branch nodes and branch criteria | Correct behavior under variation |\n| Exception recall | Capture of warnings and prohibited actions | Prevents silent hazardous omissions |\n| Traceability score | Presence of source-span evidence per extracted claim | Supports audit and review |\n\nThis split makes failures visible where aggregate metrics hide them.\n\n## Severity weighting changes decisions\n\nNot all errors are equal. Missing a unit label and missing a lockout precondition should not carry the same penalty.\n\nI recommend assigning severity classes in the gold standard:\n\n- **Critical**: omission can directly affect safety-critical action.\n- **Major**: omission can degrade decision quality or compliance.\n- **Minor**: omission has low operational consequence.\n\nThen compute a severity-weighted error index alongside base metrics.\n\nSimple form:\n\n```\nWeightedError = sum(severity_weight_i * error_i) / sum(severity_weight_i)\n```\n\nUse weights that domain experts agree on, not arbitrary values decided only by engineers.\n\n## Constraint coverage should be explicit\n\nConstraint coverage is often treated as a side metric. In my experience, it should be a primary release gate for procedural systems.\n\nPractical definition:\n\n```\nConstraintCoverage = matched_constraints / gold_constraints\n```\n\nBut matching needs discipline:\n\n- match by semantic equivalence, not only string overlap,\n- require linkage to the correct step,\n- verify scope boundaries when the same term appears in multiple sections.\n\nA condition extracted in the wrong place is not a partial success. It is a structural error.\n\n## Evaluation protocol that scales\n\nA useful protocol for applied research-to-pilot work:\n\n1. Build a gold set with mixed complexity (easy, medium, edge-case documents).\n2. Annotate steps, transitions, constraints, exceptions, and severity labels.\n3. Run extraction with fixed config and seeded decoding for reproducibility.\n4. Compute metric suite and failure taxonomy.\n5. Review error clusters with domain experts.\n6. Update chunking, prompts, or validators.\n7. Re-run on unchanged gold set before adding new data.\n\nThe key is resisting metric drift. If the gold set changes every cycle, your trend lines become hard to trust.\n\n## Failure taxonomy is as important as scores\n\nRaw numbers tell you \"how much.\" Taxonomy tells you \"why.\"\n\nUseful failure classes:\n\n- condition dropped,\n- condition mis-scoped,\n- step merge,\n- hallucinated transition,\n- exception omitted,\n- role misassignment,\n- source citation missing.\n\nWhen you track these categories over iterations, you can see whether improvements come from real behavior change or from scoring artifacts.\n\n## Add abstention quality to your dashboard\n\nA mature system should know when it is unsure.\n\nTrack:\n\n- abstention rate,\n- abstention precision (how often abstention was justified),\n- high-risk false accept rate.\n\nIn safety-adjacent workflows, a thoughtful abstention is usually better than a confident error with polished prose.\n\n## Latency and reliability still belong in evaluation\n\nEvaluation cannot ignore operational constraints. Even high-quality extraction is unusable if latency is unstable or failures are opaque.\n\nMinimum operational metrics:\n\n- p50/p95 end-to-end latency,\n- stage-level latency breakdown,\n- validation failure rate,\n- retry and timeout rates,\n- reproducibility delta across repeated runs.\n\nFor partner-facing pilots, include these in the same report as semantic metrics. Keeping them separate usually delays integration issues until late stages.\n\n## A practical release gate template\n\nBefore promoting a model/pipeline version, require all of the following:\n\n- Step F1 above target threshold,\n- constraint coverage above target threshold,\n- zero unresolved critical-severity errors on benchmark set,\n- traceability score above minimum,\n- latency SLO met at p95,\n- no regression in top failure classes over previous version.\n\nThis is stricter than typical ML dashboards. It is also closer to what industrial partners expect once real workflows depend on the system.\n\n## What I would prioritize next\n\n- Better automated scoring for condition scope correctness.\n- Shared benchmark fragments across organizations without exposing sensitive text.\n- Confidence calibration that maps to true error likelihood per error class.\n- Review tools that connect each metric regression to concrete examples immediately.\n\nEvaluation is not the final chapter after model building. In applied industrial AI, evaluation is the control loop that decides whether a prototype is trustworthy enough to become a pilot.","tags":["evaluation","industrial-ai","safety","procedural-knowledge-extraction","reliability"],"published_date":"2026-02-04T09:10:00+00:00","updated_at":"2026-02-04T09:10:00+00:00","read_time":5,"image_url":"https://miro.medium.com/1*d3PZ7JqYIy_ENB2zuUOk3Q.jpeg"},{"id":"85645218-6250-4feb-b642-94fb7a734d7f","title":"Designing Local LLM Pipelines for Industrial Documentation","slug":"designing-local-llm-pipelines-for-industrial-documentation","excerpt":"A field guide for building local document intelligence pipelines when privacy, latency, and traceability are non-negotiable constraints.","body":"Industrial documentation is a reliable stress test for LLM engineering discipline. Documents are long, formatting is inconsistent, OCR quality varies, and some of the most important content lives in footnotes, tables, or warning boxes.\n\nNow add one more constraint: data cannot leave the organization.\n\nAt that point, local LLM design stops being a convenience choice and becomes an architecture requirement.\n\n## Why local is not only a privacy preference\n\nTeams often frame local inference as \"we do not want cloud APIs.\" In practice, there are four stronger reasons:\n\n- **Data boundary control**: sensitive operational procedures and incident reports stay on approved infrastructure.\n- **Predictable latency**: no external network round-trips for every extraction call.\n- **Operational continuity**: the pipeline can continue during external outages or policy restrictions.\n- **Auditability**: model versions, prompts, and outputs are tied to internal release workflows.\n\nA local stack gives you leverage only if you treat it like infrastructure, not a demo script.\n\n## Architecture that survives real documents\n\nA robust local pipeline usually needs six modules:\n\n1. ingestion and normalization,\n2. document segmentation,\n3. retrieval and context assembly,\n4. extraction,\n5. validation,\n6. serving and monitoring.\n\nSimple diagram:\n\n```mermaid\nflowchart LR\n  A[\"Document Ingestion\"] --> B[\"OCR and Normalization\"]\n  B --> C[\"Structure-Aware Chunking\"]\n  C --> D[\"Context Retrieval\"]\n  D --> E[\"LLM Extraction\"]\n  E --> F[\"Schema and Rule Validation\"]\n  F --> G[\"Knowledge Graph or API Output\"]\n  G --> H[\"Monitoring and Human Review\"]\n```\n\nIf one of these blocks is missing, the model usually gets blamed for failures that are really pipeline failures.\n\n## Start with ingestion quality, not model selection\n\nI used to begin by swapping models and prompt templates. That was a mistake. For industrial documentation, ingestion quality dominates more than most teams expect.\n\nCritical ingestion steps:\n\n- normalize line breaks and list markers,\n- recover heading hierarchy,\n- preserve table context where thresholds and units live,\n- keep references between warning blocks and nearby steps.\n\nIf your text loses this structure early, no model fully recovers it later.\n\n## Chunking strategy is a first-class design decision\n\nFixed-size chunking is simple and often wrong for procedural text.\n\nI get better outcomes with structure-aware chunking:\n\n- split by heading and subheading boundaries,\n- keep list sequences intact,\n- allow overlap around condition-heavy regions,\n- use smaller chunk caps for dense safety sections.\n\nA good chunking policy reduces both hallucination rate and condition misattachment.\n\n## Model and runtime choices under hardware limits\n\nOn local hardware, model selection is a multi-objective tradeoff:\n\n- output quality,\n- latency,\n- memory pressure,\n- stability over long contexts.\n\nMy practical baseline is a quantized 7B-class model with conservative decoding settings. It is rarely perfect, but it is stable enough for fast iteration when paired with a strict validation layer.\n\nTypical knobs that matter in production-like settings:\n\n- context window tuned to actual chunk profile,\n- temperature kept low for structured extraction,\n- output token caps that avoid runaway responses,\n- deterministic seeds for reproducible comparisons.\n\n## Validation is where trust is built\n\nWithout validation, local LLM output is just untrusted text with better formatting.\n\nFor procedural extraction, I require:\n\n- schema validity,\n- required fields for every step and constraint,\n- edge sanity checks (no broken references),\n- source-span evidence for critical constraints.\n\nWhen validation fails, the pipeline should not silently \\\"best effort\\\" the output. It should either repair with explicit rules or abstain and request review.\n\nThat abstain path looks less impressive in demos and is usually much safer in real operations.\n\n## Monitoring beyond uptime\n\nMost teams monitor service health and call it done. For document intelligence, monitor semantic drift too.\n\nUseful counters:\n\n- extraction success rate per document type,\n- average constraints per document over time,\n- validation failure classes,\n- manual correction rate in review UI,\n- latency percentiles by stage.\n\nIf correction rate spikes after a format change from a partner team, you want to detect it in hours, not months.\n\n## Human review should be designed, not bolted on\n\nPeople will review outputs anyway. The question is whether your interface helps them do it quickly and safely.\n\nA good review surface should show:\n\n- extracted step,\n- linked constraint,\n- exact source snippet,\n- reason for validation warning,\n- one-click correction path.\n\nWhen review takes too long, teams silently skip it. Then confidence decays and adoption stalls.\n\n## Common mistakes I see in local deployments\n\n- treating OCR as solved when table extraction is still brittle,\n- evaluating only on small clean samples,\n- overfitting prompts to one document family,\n- logging too little metadata to debug failures later,\n- ignoring versioning for prompts and chunking policies.\n\nNone of these issues are glamorous, but each one creates avoidable rework.\n\n## What changes when moving from prototype to pilot\n\nThe biggest shift is accountability. In prototype mode, you optimize for \"can it work.\" In pilot mode, you optimize for \"can we explain and maintain it.\"\n\nThat means:\n\n- explicit data contracts,\n- traceable release notes,\n- rollback paths,\n- clear ownership for model, pipeline, and validation rules.\n\nIf your system cannot explain why it produced a specific constraint mapping, it is not ready for partner-facing work.\n\n## Open problems worth solving next\n\n- Better local layout parsers for mixed text-table-warning documents.\n- Confidence calibration for condition extraction under OCR noise.\n- Lightweight active learning loops using reviewer corrections.\n- Stronger detection of contradictory constraints across document versions.\n\nLocal LLM pipelines are not a compromise when they are designed well. In industrial settings, they are often the architecture that makes deployment possible in the first place.","tags":["local-llm","industrial-ai","privacy-by-design","systems-engineering","architecture"],"published_date":"2026-01-16T07:45:00+00:00","updated_at":"2026-01-16T07:45:00+00:00","read_time":5,"image_url":"https://substackcdn.com/image/fetch/$s_!mMj9!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1dd3c4b8-031a-469f-a0c1-6c052b5d3f58_2562x1231.png"},{"id":"38c5af12-e571-447e-8a54-e330254679b6","title":"Procedural Knowledge Extraction Is Not Summarization","slug":"procedural-knowledge-extraction-is-not-summarization","excerpt":"In industrial documentation, a fluent summary can still be operationally wrong. This note explains why procedural extraction needs structure, constraints, and traceable validation.","body":"Most teams start procedural extraction with a familiar baseline: summarize each page, stitch the text, and hope the model has captured the process. I did the same at the beginning. The output looked polished, but it failed in the exact places that matter most.\n\nThat mismatch was one of the first uncomfortable lessons I learned while working with industrial manuals. If the goal is operational support, style quality is almost irrelevant. What matters is whether the system preserves executable logic: order, conditions, exceptions, and responsibility boundaries.\n\nA summary can be fluent and still hide a safety-critical branch.\n\n## A concrete failure pattern\n\nTake a typical maintenance instruction:\n\n1. Isolate power.\n2. Confirm lockout tag status.\n3. If pressure is above threshold, vent line before valve replacement.\n4. Replace valve.\n5. Record torque value and sign-off.\n\nA standard summary usually compresses this into a paragraph about \"safe valve replacement.\" It might keep the broad intent and still lose the conditional step about pressure and venting. To a reader, that omission can look minor. In a plant, it is not minor.\n\nThat is why I treat procedural extraction as a structured prediction problem, not a text compression problem.\n\n## What summarization misses\n\nThe recurring misses are consistent across domains:\n\n- **Condition fidelity**: thresholds, guards, and preconditions disappear first.\n- **Decision points**: branch logic gets flattened into linear prose.\n- **Exception handling**: warnings, prohibited actions, and fallback paths are dropped.\n- **Role binding**: who approves, who executes, and who verifies becomes vague.\n\nIf your downstream system is a search UI, you might get away with this. If your downstream system informs real operations, you cannot.\n\n## What to extract instead\n\nThe extraction target should look like process logic, not paragraph logic. In practice, that means a graph-oriented schema with explicit types.\n\nMinimal example:\n\n```json\n{\n  \"steps\": [\n    {\"id\": \"S1\", \"action\": \"Isolate power\"},\n    {\"id\": \"S2\", \"action\": \"Confirm lockout tag status\"},\n    {\"id\": \"S3\", \"action\": \"Vent line\", \"condition\": \"pressure > threshold\"},\n    {\"id\": \"S4\", \"action\": \"Replace valve\"}\n  ],\n  \"edges\": [\n    {\"type\": \"NEXT\", \"from\": \"S1\", \"to\": \"S2\"},\n    {\"type\": \"CONDITIONAL_NEXT\", \"from\": \"S2\", \"to\": \"S3\", \"if\": \"pressure > threshold\"},\n    {\"type\": \"NEXT\", \"from\": \"S3\", \"to\": \"S4\"}\n  ],\n  \"constraints\": [\n    {\"type\": \"SAFETY_GUARD\", \"step_id\": \"S3\", \"text\": \"vent before replacement\"}\n  ]\n}\n```\n\nThis is not about graph databases for their own sake. The graph is useful because it makes omissions visible. You can inspect missing guards, broken edges, and orphaned conditions directly.\n\n## Pipeline design that works better\n\nThe extraction stack that has held up best for me has four explicit phases:\n\n1. **Structure-aware chunking**: preserve headings, list boundaries, and procedural block integrity.\n2. **Step extraction first**: force a stable ordered skeleton before attempting constraint attachment.\n3. **Constraint linking second**: map warnings, thresholds, and exceptions to specific step IDs.\n4. **Validation and repair pass**: run schema checks and lightweight consistency rules before publish.\n\nThe reason this staged setup works is simple: it reduces simultaneous ambiguity. If a model tries to infer order, condition scope, and relationship types in one pass, you usually get elegant but fragile output.\n\n## Evaluation needs to match operational risk\n\nAccuracy alone is a weak signal here. You can score high on generic overlap metrics and still fail the safety logic.\n\nI recommend tracking at least these dimensions:\n\n- **Step F1**: are core actions captured.\n- **Adjacency fidelity**: are transitions and ordering preserved.\n- **Constraint coverage**: what share of required guards and conditions are recovered.\n- **Decision-point fidelity**: are branch nodes and branch criteria retained.\n- **Exception recall**: are explicit warnings and prohibited actions represented.\n\nIf I had to choose one \"do not skip\" metric, it is constraint coverage. In industrial procedures, missing constraints cause disproportionate downstream risk.\n\n## Failure modes worth planning for\n\nThree failure modes show up repeatedly even with strong prompts:\n\n- **Scope leakage**: a condition from one subsection gets attached to the wrong step in another subsection.\n- **False merge**: two similar steps are collapsed into one generalized action.\n- **Hallucinated bridge**: the model inserts a transition that sounds plausible but does not exist in the source.\n\nThe mitigations are not glamorous, but they are effective:\n\n- keep chunk windows smaller around dense condition blocks,\n- use explicit step IDs during extraction,\n- require source-span references for every extracted constraint,\n- reject outputs that fail graph sanity checks.\n\n## Why this matters beyond one thesis project\n\nThere is a broader lesson here for applied AI in industry. Fluency can hide structural error. The more polished the output, the easier it is for teams to over-trust it.\n\nFor decision support, we need systems that are inspectable before they are impressive.\n\nThat is why my default target is not \"best sounding answer.\" It is an auditable intermediate representation that an engineer can question, debug, and improve over time.\n\n## Open problems I am actively interested in\n\n- Better automatic detection of condition scope boundaries across noisy OCR text.\n- Severity-aware scoring so missing high-risk constraints is penalized more heavily.\n- Hybrid extraction with rules plus model inference for stricter guard recovery.\n- Operator-friendly review interfaces that allow fast correction without schema expertise.\n\nIf you are building in this space, start with a small, high-quality gold set and one hard requirement: every extracted constraint must point back to source evidence. In my experience, that single decision improves system quality more than most prompt-tuning tricks.","tags":["procedural-knowledge-extraction","industrial-ai","knowledge-graphs","evaluation","safety"],"published_date":"2025-12-29T08:20:00+00:00","updated_at":"2025-12-29T08:20:00+00:00","read_time":5,"image_url":"https://miro.medium.com/v2/resize:fit:2000/format:webp/1*jNWQdkJwTMSN4P9_OvqBsQ.png"},{"id":"19f314c2-c0fe-4581-8e16-e915a9f0d62f","title":"Designing a Small Local LLM Pipeline on Apple Silicon: Engineering Notes","slug":"local-llm-apple-silicon-engineering-notes","excerpt":"A practical engineering note on building and evaluating a local transformer pipeline under tight hardware constraints, including failure modes and reliability tradeoffs.","body":"## The Spark That Started It All\n\nI'll be honest with you, six months ago, I thought building a language model was something only PhD researchers at big tech companies could do. I mean, we're talking about neural networks that can understand and generate human language, right? That's got to be rocket science.\n\nBut then I stumbled across a YouTube video of someone training a tiny GPT model on Shakespeare's works, and something just clicked. \"What if I could build my own?\" I thought. Not something that would compete with ChatGPT or Claude, but something *mine*—something I could understand from the ground up.\n\nSpoiler alert: it was harder than I expected, but also way more rewarding than I imagined.\n\n## Why I Decided to Go Local (And Small)\n\nBefore diving into the technical stuff, let me tell you why I chose to build a small, local model instead of just fine-tuning an existing one.\n\nFirst, **privacy**. I wanted something that would run entirely on my machine, no data leaving my computer. Second, **understanding**. I'm the kind of person who needs to know how things work under the hood. And third, **constraints breed creativity**. With my M4 MacBook Pro and its 16GB of unified memory, I had to get creative about architecture and training. No fancy GPU here—just the neural engine and whatever PyTorch could squeeze out of Metal Performance Shaders.\n\n## The Technical Journey\n\n### Step 1: Understanding the Basics\n\nI started with the fundamentals. If you're thinking about doing this yourself, don't skip this part like I almost did. I spent two weeks just reading papers and tutorials:\n\n- The original \"Attention Is All You Need\" paper (read it three times before it clicked)\n- Andrej Karpathy's \"makemore\" series (absolute gold)\n- The nanoGPT repository \n\n**Pro tip**: Don't try to understand everything at once. I made notes in a simple text file, and looking back, some of my early notes make me laugh. \"Attention = somehow the model looks at different parts of the input\" was one of my profound insights.\n\n### Step 2: Choosing My Architecture\n\nI went with a decoder-only transformer architecture, similar to GPT but much smaller. Here's what I settled on:\n\n- **6 layers** (compared to GPT-3's 96)\n- **8 attention heads** \n- **384 embedding dimensions**\n- **Vocabulary size: 10,000** tokens\n- **Context window: 512** tokens\n\nTotal parameters? About 10 million. Tiny by today's standards, but perfect for my laptop.\n\n### Step 3: The Data Dilemma\n\nThis was my first real challenge. What do you train a small model on? I considered several options:\n\n- **Wikipedia dumps** (too big, too varied)\n- **Books** (copyright issues)\n- **My own writing** (not enough data)\n\nI ended up creating a curated dataset mixing:\n- Public domain books from Project Gutenberg\n- Wikipedia articles on topics I cared about\n- Programming tutorials and documentation\n- Some Reddit discussions (carefully filtered)\n\nFinal dataset: about 50MB of text. Small, but focused.\n\n### Step 4: Implementation Reality Check\n\nI initially planned to write everything from scratch in pure Python. That lasted about two days. Here's what I actually ended up using:\n\n- **PyTorch** for the neural network (with MPS backend for M4 acceleration)\n- **Hugging Face tokenizers** for text preprocessing\n- **Weights & Biases** for experiment tracking\n- **A lot of Stack Overflow** for debugging Metal Performance Shaders issues\n\nThe actual model implementation was about 200 lines of Python. Here's the surprising part: getting PyTorch to properly utilize the M4's neural engine took longer than writing the model itself. Apple's Metal Performance Shaders documentation became my bedtime reading.\n\n### Step 5: Training Adventures\n\nMy first training run was a disaster. The loss went to NaN after 100 steps, and I had no idea why. After some debugging (and discovering that MPS has some quirks with certain operations), I found out my learning rate was way too high and I needed to move some operations back to CPU.\n\n**Training specs:**\n- **Batch size**: 8 (memory constraints with 16GB shared between system and model)\n- **Learning rate**: 3e-4 (after much experimentation)\n- **Training time**: 2 days on M4 (surprisingly efficient!)\n- **Final loss**: 2.4 (not great, but not terrible)\n\nThe M4 was actually fantastic for this kind of work. The unified memory architecture meant I could load larger datasets than I expected, and the neural engine handled the matrix operations beautifully once I got the MPS backend working properly.\n\nI trained three different versions, each time learning something new:\n1. **Version 1**: Overfit to the training data\n2. **Version 2**: Added dropout, but learning rate was still wrong\n3. **Version 3**: Finally got it right\n\n## The Moment of Truth\n\nAfter two days of training version 3, I nervously typed my first prompt: \"The future of artificial intelligence\"\n\nThe output was... underwhelming:\n> The future of artificial intelligence is a complex and multifaceted topic that involves many different aspects of technology and society. In recent years, there has been...\n\nIt was generic, but it was *coherent*. My little model was actually generating reasonable text! I may have done a small victory dance in my room. (The M4 stayed remarkably cool throughout this whole process, which was a nice bonus.)\n\n## What I Learned (The Hard Way)\n\n### Technical Lessons\n\n1. **Learning rate scheduling matters more than I thought**. I spent hours tweaking the architecture when the real problem was my learning rate decay.\n\n2. **Data quality beats data quantity**. My 50MB of curated text worked better than the 500MB of random internet text I tried first.\n\n3. **Regularization is crucial**. Without proper dropout and weight decay, my model would memorize the training data instead of learning patterns.\n\n### Personal Lessons\n\n1. **Start smaller than you think**. My first attempt had 50 million parameters. My successful model had 10 million.\n\n2. **Document everything**. I wish I'd kept better notes about what worked and what didn't. Future me would thank past me.\n\n3. **The community is amazing**. I got help from strangers on Reddit, Discord, and Twitter. The ML community really wants to help newcomers.\n\n## Performance and Limitations\n\nLet's be real about what my little model can and can't do:\n\n**What it's good at:**\n- Writing coherent paragraphs on familiar topics\n- Completing sentences in a reasonable way\n- Following basic instruction patterns\n\n**What it struggles with:**\n- Complex reasoning\n- Staying on topic for long passages\n- Anything requiring real-world knowledge\n- Math (it once told me 2+2=5, very confidently)\n\n## Lessons for Anyone Thinking About This\n\nIf you're considering building your own LLM, here's my honest advice:\n\n**Do it if:**\n- You want to understand how these models really work\n- You enjoy debugging and experimentation\n- You have a specific use case in mind\n- You learn better by doing than reading\n\n**Don't expect:**\n- To build the next ChatGPT\n- It to be easy or quick\n- Perfect results on your first try\n- To save money compared to using APIs\n\n## What's Next?\n\nI'm already planning version 4. I want to experiment with:\n- **Mixture of Experts** architecture\n- **Better tokenization** strategies\n- **Instruction tuning** on my own dataset\n- **Quantization** to make it even more efficient\n\nBut honestly? The biggest win isn't the model itself—it's the understanding I gained. Every time I use ChatGPT or Claude now, I have a much better appreciation for what's happening under the hood.\n\n## Final Thoughts\n\nBuilding my own LLM was like learning to cook by growing your own vegetables. Sure, you can buy better results at the store, but there's something magical about the process. Every bug I fixed, every hyperparameter I tuned, every coherent sentence my model generated—it all felt like a small victory.\n\nIf you're curious about AI and have some programming experience, I'd encourage you to try it. Not because you'll build something revolutionary, but because you'll understand something revolutionary in a completely new way.\n\nAnd who knows? Maybe your small model will surprise you, just like mine surprised me.","tags":["local-llm","industrial-ai","evaluation","privacy-by-design","systems-engineering"],"published_date":"2025-07-11T21:07:09.397+00:00","updated_at":"2026-02-09T12:18:27.145+00:00","read_time":7,"image_url":"https://images.manning.com/book/1/92362be-2647-4fa1-9998-e4959d9c568f/DOTD_Raschka.png"}];</script>
</body>
</html>
