<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <!-- Meta tags dynamically injected by <Seo /> component via react-helmet-async -->
    <!-- Static fallbacks only for crawlers that don't execute JS -->
    <title>Projects | Imadlab | Research Engineer &amp; Internal CTO</title>
    <meta name="description" content="Selected applied research projects in multimodal industrial AI, procedural knowledge extraction, and deployable data systems." />
    <!-- prerender-seo:start -->
    <link rel="canonical" href="https://imadlab.me/projects" />
    <meta property="og:title" content="Projects | Imadlab | Research Engineer &amp; Internal CTO" />
    <meta property="og:description" content="Selected applied research projects in multimodal industrial AI, procedural knowledge extraction, and deployable data systems." />
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://imadlab.me/projects" />
    <meta property="og:image" content="https://imadlab.me/images/hero-moon.png" />
    <meta property="og:image:alt" content="Projects | Imadlab | Research Engineer &amp; Internal CTO" />
    <meta property="og:locale" content="en_US" />
    <meta property="og:site_name" content="Imadlab" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:site" content="@imadlab" />
    <meta name="twitter:creator" content="@imadlab" />
    <meta name="twitter:title" content="Projects | Imadlab | Research Engineer &amp; Internal CTO" />
    <meta name="twitter:description" content="Selected applied research projects in multimodal industrial AI, procedural knowledge extraction, and deployable data systems." />
    <meta name="twitter:image" content="https://imadlab.me/images/hero-moon.png" />
    <meta name="twitter:image:alt" content="Projects | Imadlab | Research Engineer &amp; Internal CTO" />
    <script type="application/ld+json">{"@context":"https://schema.org","@type":"CollectionPage","name":"Projects","url":"https://imadlab.me/projects"}</script>
    <!-- prerender-seo:end -->

    <!-- Favicon and Theme Color -->
    <link rel="icon" href="/favicon.ico" sizes="any">
    <link rel="apple-touch-icon" href="/apple-touch-icon.png">
    <meta name="theme-color" content="#000000" />

    <!-- Preload hero image for faster LCP on home route -->
    <link rel="preload" as="image" href="/images/hero-moon.avif" type="image/avif" />

    <!-- Font Preconnect for Performance -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    
    <!-- Load fonts with font-display: swap for better performance -->
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800;900&family=JetBrains+Mono:wght@300;400;500;600;700&display=swap" rel="stylesheet" media="print" onload="this.media='all'">
    <noscript>
      <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800;900&family=JetBrains+Mono:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    </noscript>
    
    <!-- DNS prefetch for Supabase and Cloudflare Analytics -->
    <link rel="dns-prefetch" href="https://mpkgugcasxpanhrkpkhs.supabase.co">
    <link rel="preconnect" href="https://mpkgugcasxpanhrkpkhs.supabase.co" crossorigin>
    <link rel="dns-prefetch" href="https://static.cloudflareinsights.com">
    <link rel="preconnect" href="https://static.cloudflareinsights.com" crossorigin>
    
    <!-- RSS/JSON Feeds -->
    <link rel="alternate" type="application/rss+xml" title="Imadlab Blog RSS Feed" href="/feed.xml" />
    <link rel="alternate" type="application/feed+json" title="Imadlab Blog JSON Feed" href="/feed.json" />
    <meta name="referrer" content="strict-origin-when-cross-origin" />

    <!-- Structured Data -->
    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "WebSite",
      "url": "https://imadlab.me",
      "name": "Imadlab",
      "description": "Applied research and engineering work by Imad Eddine El Mouss on multimodal industrial AI, procedural knowledge extraction, and deployable systems.",
      "publisher": {
        "@type": "Person",
        "name": "Imad Eddine El Mouss",
        "url": "https://imadlab.me/about",
        "sameAs": [
          "https://github.com/imaddde867",
          "https://www.linkedin.com/in/imad-eddine-e-986741262"
        ]
      },
      "potentialAction": {
        "@type": "SearchAction",
        "target": "https://duckduckgo.com/?q=site%3Aimadlab.me+{search_term_string}",
        "query-input": "required name=search_term_string"
      }
    }
    </script>
    <script type="module" crossorigin src="/assets/js/index-B1IGmRtD.js"></script>
    <link rel="stylesheet" crossorigin href="/assets/index-9eB0Q-OK.css">
  </head>

  <body>
    <h1 style="position: absolute; width: 1px; height: 1px; padding: 0; margin: -1px; overflow: hidden; clip: rect(0, 0, 0, 0); border: 0;">Imadlab | Research Engineer & Internal CTO</h1>
    <div id="root">
<main data-prerender="true" class="prerender-shell">
  <h1 class="text-3xl font-bold mb-4">Projects</h1>
  <section class="prerender-grid">

    <article class="prerender-card">
      <h2 class="prerender-title">Multi-View Industrial Context Tracking</h2>
      <p class="prerender-tags">Tech: industrial-ai, computer-vision, multimodal-context, cross-camera-tracking, pytorch</p>
      <p class="prerender-summary">End-to-end multi-camera perception prototype with auto-labeling, detector training, and cross-camera identity consistency for operational context awareness.</p>
      <a href="/projects/bb3b627c-3e6e-46e1-ada8-a0764e991a66" class="prerender-link">View project</a>
    </article>

    <article class="prerender-card">
      <h2 class="prerender-title">IPKE: Procedural Knowledge Graph Extraction from Industrial Documentation</h2>
      <p class="prerender-tags">Tech: industrial-ai, procedural-knowledge-extraction, knowledge-graphs, local-llm, privacy-by-design</p>
      <p class="prerender-summary">Privacy-by-Design local LLM pipeline that extracts procedural steps, constraints, and decision points from industrial documents into auditable knowledge graphs.</p>
      <a href="/projects/b2aedbfa-caaa-4837-bcd2-0ad236aa8922" class="prerender-link">View project</a>
    </article>

    <article class="prerender-card">
      <h2 class="prerender-title">NaviCast: Real-Time Maritime Intelligence Pipeline</h2>
      <p class="prerender-tags">Tech: real-time-systems, data-pipelines, time-series, predictive-modeling, mqtt</p>
      <p class="prerender-summary">Real-time edge-to-cloud pipeline combining AIS ingestion, validated storage, and trajectory forecasting for operational decision support.</p>
      <a href="/projects/1139086b-79f9-44a0-8f81-145dcc137b74" class="prerender-link">View project</a>
    </article>

    <article class="prerender-card">
      <h2 class="prerender-title">Vehicle Detection and Stylization Benchmark (GAN-CNN)</h2>
      <p class="prerender-tags">Tech: computer-vision, object-detection, model-optimization, yolov5, tensorflow</p>
      <p class="prerender-summary">Computer vision benchmark from labeled detection data to deployable models with reproducible training artifacts and model export.</p>
      <a href="/projects/5aae3db6-ac22-487a-9822-bbffc27833e9" class="prerender-link">View project</a>
    </article>

    <article class="prerender-card">
      <h2 class="prerender-title">Spiral Untangler: From-Scratch Neural Network in NumPy</h2>
      <p class="prerender-tags">Tech: ml-fundamentals, from-scratch-ml, numpy, classification</p>
      <p class="prerender-summary">Compact from-scratch neural network experiment focused on nonlinear decision boundaries, training diagnostics, and model behavior inspection.</p>
      <a href="/projects/3d321db4-bd68-42d1-beae-f456341eb76e" class="prerender-link">View project</a>
    </article>

    <article class="prerender-card">
      <h2 class="prerender-title">InfiniteChessAI (Archived Prototype)</h2>
      <p class="prerender-tags">Tech: archived, prototype</p>
      <p class="prerender-summary">Early exploratory prototype, archived to keep the public portfolio focused on industrial research work.</p>
      <a href="/projects/674332e4-f4aa-48ef-9d51-e66ffd0b7f04" class="prerender-link">View project</a>
    </article>

    <article class="prerender-card">
      <h2 class="prerender-title">Federated Learning Privacy Benchmark: Gradient Leakage and Defenses</h2>
      <p class="prerender-tags">Tech: privacy-by-design, federated-learning, security-evaluation, gradient-inversion, pytorch</p>
      <p class="prerender-summary">Reproducible benchmark measuring gradient inversion risk and the protection impact of differential privacy and homomorphic encryption.</p>
      <a href="/projects/6dd91268-b428-4803-9100-904bf51b895f" class="prerender-link">View project</a>
    </article>

    <article class="prerender-card">
      <h2 class="prerender-title">Term Deposit Subscription Prediction (Tabular ML Benchmark)</h2>
      <p class="prerender-tags">Tech: tabular-ml, classification, feature-engineering, tensorflow, scikit-learn</p>
      <p class="prerender-summary">Tabular ML pipeline for propensity prediction with feature engineering, leakage-aware modeling, and reproducible evaluation.</p>
      <a href="/projects/80b51708-efa4-4ab8-a97e-9a61ea3bdefc" class="prerender-link">View project</a>
    </article>

    <article class="prerender-card">
      <h2 class="prerender-title">imadlab.me: Research Portfolio Platform</h2>
      <p class="prerender-tags">Tech: platform-engineering, react, supabase, content-management, analytics</p>
      <p class="prerender-summary">Personal research platform with structured project/blog publishing, Supabase-backed content operations, and analytics instrumentation.</p>
      <a href="/projects/5cf746e6-934f-400d-b51b-62e0daa4a183" class="prerender-link">View project</a>
    </article>

    <article class="prerender-card">
      <h2 class="prerender-title">ClearBox: Secure Messaging Platform Prototype</h2>
      <p class="prerender-tags">Tech: secure-systems, messaging, fastapi, react, postgresql</p>
      <p class="prerender-summary">Secure messaging prototype exploring real-time communication, access control boundaries, and deployable service architecture.</p>
      <a href="/projects/51d8de9b-5876-4eda-ac5b-3411814bda84" class="prerender-link">View project</a>
    </article>

    <article class="prerender-card">
      <h2 class="prerender-title">Spotify Recommendation Pipeline (Content-Based Clustering)</h2>
      <p class="prerender-tags">Tech: recommendation-systems, ml-pipelines, flask, feature-engineering, clustering</p>
      <p class="prerender-summary">Recommendation system prototype combining feature engineering, clustering, and retrieval strategies over large audio metadata collections.</p>
      <a href="/projects/ea66518d-ced2-4d09-b84c-63f57eefd644" class="prerender-link">View project</a>
    </article>
  </section>
</main>
</div>
    
  <script>window.__PRERENDERED_DATA__ = window.__PRERENDERED_DATA__ || {}; window.__PRERENDERED_DATA__["projects"] = [{"id":"bb3b627c-3e6e-46e1-ada8-a0764e991a66","title":"Multi-View Industrial Context Tracking","description":"End-to-end multi-camera perception prototype with auto-labeling, detector training, and cross-camera identity consistency for operational context awareness.","full_description":"This project is my full-stack take on multi-view perception: data engineering for large-scale labeling, model training, and robust cross-camera tracking. The pipeline is config-driven and reproducible end-to-end using a CLI workflow (`multiview label | train | run`).\n\n**Pipeline**\n1. **Labeling**: Sample synchronized frames across camera groups, propose boxes with YOLO, then refine masks with SAM3.\n2. **Training**: Train a YOLO detector on the generated dataset.\n3. **System run**: Detect and track per camera, then associate tracks across cameras to assign global IDs.\n\n**Dataset and Training Snapshot**\n| Item | Value |\n| --- | --- |\n| Source videos | 7 synchronized cameras |\n| Images labeled | 3,104 |\n| Total labels | 34,198 |\n| Labels per image | 11.0 |\n| Classes | person, car, bus |\n| Frame stride | 5 |\n| Base model | YOLO11m |\n| Train config | 100 epochs, 960 imgsz, batch 16 |\n\n**Multi-View Tracking Snapshot (Group g34)**\n| Item | Value |\n| --- | --- |\n| Frames | 1,757 |\n| Cameras | cam3 + cam4 |\n| Total detections | 38,253 |\n| Avg detections per frame | 21.8 |\n| Global IDs | 161 (70 person, 91 car) |\n| Class counts | 9,468 person, 28,785 car |\n\n**Visual Evidence and Artifacts**\n![SAM3 auto-labeling visualization](https://raw.githubusercontent.com/imaddde867/Multi-view-object-detection/main/data/processed/showcase/sam3_autolabel_v2/viz/train/Cam3_f000025.jpg)\n*SAM3-refined labels over YOLO proposals from the auto-labeling run.*\n\n![Training batch preview](https://raw.githubusercontent.com/imaddde867/Multi-view-object-detection/main/results/showcase/training/sam3_autolabel_v2/train_batch0.jpg)\n*Training samples from the YOLO pipeline.*\n\n![Validation predictions](https://raw.githubusercontent.com/imaddde867/Multi-view-object-detection/main/results/showcase/training/sam3_autolabel_v2/val_batch0_pred.jpg)\n*Validation predictions from the trained model.*\n\n![Confusion matrix](https://raw.githubusercontent.com/imaddde867/Multi-view-object-detection/main/results/showcase/training/sam3_autolabel_v2/confusion_matrix_normalized.png)\n*Normalized confusion matrix for the trained detector.*\n\n![Training curves](https://raw.githubusercontent.com/imaddde867/Multi-view-object-detection/main/results/showcase/training/sam3_autolabel_v2/results.png)\n*Training curves from the YOLO run.*\n\n![Precision/Recall trend](https://raw.githubusercontent.com/imaddde867/Multi-view-object-detection/main/results/showcase/training/sam3_autolabel_v2/BoxF1_curve.png)\n*F1 score trend across epochs.*\n\nDemo video:\n\u003Cvideo controls src=\"https://raw.githubusercontent.com/imaddde867/Multi-view-object-detection/main/results/showcase/system/sam3_autolabel_v2/g34_demo.mp4\" poster=\"https://raw.githubusercontent.com/imaddde867/Multi-view-object-detection/main/results/showcase/training/sam3_autolabel_v2/val_batch0_pred.jpg\">\u003C/video>\n\n**Engineering Highlights**\n- **Reproducibility**: Config-driven runs, tracked artifacts under `results/showcase/`.\n- **Scalable labeling**: SAM3 refinement generates training data quickly without manual annotation.\n- **Cross-camera consistency**: Global ID assignment keeps identities stable across camera views.\n- **Deployment-ready**: CLI pipeline supports CPU/GPU toggles and SLURM job scripts for HPC runs.\n\n**Try it**\n```bash\nmultiview label --config config/labeling.yaml\nmultiview train --config config/train.yaml\nmultiview run --config config/system_demo_tuned.yaml\n```","tech_tags":["industrial-ai","computer-vision","multimodal-context","cross-camera-tracking","pytorch","mlflow"],"created_at":"2025-12-24T19:03:27.022547+00:00","updated_at":"2026-02-09T12:18:27.145+00:00","image_url":"https://imadlab.me/images/projects/multi-view-industrial-context-tracking.gif","repo_url":"https://github.com/imaddde867/Multi-view-object-detection","demo_url":null,"featured":true},{"id":"b2aedbfa-caaa-4837-bcd2-0ad236aa8922","title":"IPKE: Procedural Knowledge Graph Extraction from Industrial Documentation","description":"Privacy-by-Design local LLM pipeline that extracts procedural steps, constraints, and decision points from industrial documents into auditable knowledge graphs.","full_description":"Thesis: https://www.theseus.fi/handle/10024/907018\n\nThis project focuses on reliable, local extraction of procedural structure from safety-critical manuals: ordered steps, entities/resources, and constraints (e.g., guards and warnings). The core idea is to reduce schema drift by combining Dual Semantic Chunking (DSC) with P3 (Two-Stage Decomposition): first extract an ordered step list, then attach constraints to the correct steps. Outputs can be visualized as interactive graphs and stored in Neo4j via a containerized deployment. An evaluation suite computes step/adjacency F1, constraint coverage, and a composite Procedural Fidelity score ($\\Phi = 0.5\\cdot\\text{ConstraintCoverage} + 0.3\\cdot\\text{StepF1} + 0.2\\cdot\\text{Kendall}$).\n\nCore math snapshot from [thesis](https://www.theseus.fi/handle/10024/907018):\n\n| Metric | Formula |\n|---|---|\n| DSC cohesion | $\\displaystyle \\text{DSC cohesion}(i,j) = \\frac{1}{j-i} \\sum_{k=i}^{j-1} \\cos(e_k, e_{k+1})$ |\n| Segmentation objective | $\\displaystyle \\text{Segmentation objective} = \\sum_{\\text{segments}} \\text{cohesion}(i,j) - \\lambda K + \\beta \\mathbb{I}[\\text{split after heading}],$\u003Cbr/>$\\beta=0.2$ |\n| ConstraintCoverage | $\\displaystyle \\text{ConstraintCoverage} = \\frac{\\text{matched\\_constraints}}{\\text{gold\\_constraints}}$ |\n| StepF1 matching rule | $\\displaystyle \\text{StepF1: Hungarian 1:1 alignment on cosine similarity, accept match if } \\text{sim} \\ge \\tau,$\u003Cbr/>$\\tau=0.75$ |\n| Procedural Fidelity | $\\displaystyle \\Phi = 0.5\\cdot\\text{ConstraintCoverage} + 0.3\\cdot\\text{StepF1} + 0.2\\cdot\\text{Kendall}$ |\n\n## Pipeline or Architecture\n1. Document ingestion (PDF/DOCX/PPTX/TXT) and text normalization.\n2. Chunking (DSC / fixed / breakpoint) to keep chunks coherent under mid-sized LLM context limits.\n3. LLM-based extraction using pluggable prompting strategies (P3, zero-shot, few-shot, chain-of-thought).\n4. Schema + constraint validation to keep outputs queryable and consistent.\n5. PKG build + canonicalization (NEXT/parallel/gateway relations, GUARD attachments for safety rules).\n6. Serving + visualization via Streamlit, FastAPI (`/extract`, `/config`, `/stats`), and Neo4j (`docker-compose.yml`).\n\n## Dataset/Training Snapshot\nGold labels shipped in `datasets/archive/gold_human/*.json`:\n\n| Document | Steps | Constraints | NEXT edges | Gateways | Parallel groups | Sentence span coverage |\n|---|---:|---:|---:|---:|---:|---:|\n| 3M_OEM_SOP | 29 | 0 | 28 | 2 | 0 | 24 / 86 (0.279) |\n| DOA_Food_Man_Proc_Stor | 40 | 3 | 11 | 1 | 10 | 40 / 641 (0.062) |\n| op_firesafety_guideline | 31 | 8 | 12 | 1 | 3 | 40 / 156 (0.256) |\n| Total | 100 | 11 | 51 | 4 | 13 | 104 / 883 (0.118) |\n\nInference configuration defaults (from `.env.example`, `models/llm/m4_optimization.json`, and `Thesis_IPKE.docx`):\n\n| Setting | Value |\n|---|---|\n| LLM | Mistral-7B-Instruct-v0.2 (GGUF) |\n| Quantization | Q4_K_M |\n| Inference backend | llama.cpp (via llama-cpp-python) |\n| Embeddings | SBERT (all-mpnet-base-v2) |\n| Context window | 8192 (default), 4096 (Apple preset) |\n| Temperature | 0.1 |\n| Max output tokens | 1536 |\n| GPU backend | auto (metal/cuda/cpu) |\n| GPU layers | -1 (all layers) |\n| Chunk size | 2000 |\n| Threads (Apple preset) | 8 |\n| Batch size (Apple preset) | 4 |\n| Max workers | 8 |\n| Random seed | 42 |\n\n## Evaluation/Tracking Snapshot\nKey thesis results summarized in `README.md`:\n\n| Method | Scope | Step F1 | Constraint coverage | Phi |\n|---|---|---:|---:|---:|\n| P3 (Two-Stage Decomposition) | Tier-A documents | 0.377 | 0.708 | 0.611 |\n| Llama-3.1-70B (zero-shot) | 3M SOP | - | 0.000 | 0.187 |\n| P3 (Two-Stage Decomposition) | 3M SOP | - | 0.750 | 0.699 |\n| P3 (earlier setup) | 3M SOP | - | 0.500 | 0.439 |\n\n## Visual Evidence and Artifacts\n![PKG schema](https://raw.githubusercontent.com/imaddde867/IPKE/main/assets/figure_pkg_schema.png)\n\n![Extracted PKG example graph (3M OEM SOP)](https://raw.githubusercontent.com/imaddde867/IPKE/main/assets/graph_example.png)\n\n![Efficiency frontier (Phi)](https://raw.githubusercontent.com/imaddde867/IPKE/main/assets/efficiency_frontier_phi.png)\n\n![Chunking comparison](https://raw.githubusercontent.com/imaddde867/IPKE/main/assets/chunking_comparison_chart.png)\n\n![Prompting comparison](https://raw.githubusercontent.com/imaddde867/IPKE/main/assets/prompting_comparison_chart.png)\n\n## Engineering Highlights\n- On-prem, privacy-preserving inference via quantized GGUF models (no external APIs required).\n- Reproducible experiment harness for chunking/prompting sweeps (`scripts/experiments/*`) plus plotting scripts that generate publication-grade figures (`scripts/plot_*.py`).\n- Clean separation of concerns: chunkers, prompting strategies, graph building, validation, and evaluation metrics live in dedicated modules under `src/`.\n- Dual interfaces: Streamlit for interactive exploration and FastAPI for programmatic extraction (`/extract` + OpenAPI docs).\n- Neo4j integration via `docker-compose.yml` for queryable PKG storage and downstream decision-support use.\n\n## Demo Video\n\u003Cvideo controls src=\"https://raw.githubusercontent.com/imaddde867/IPKE/main/assets/demo.mp4\" poster=\"https://raw.githubusercontent.com/imaddde867/IPKE/main/datasets/Samples/UI_Frontend_screenshot.png\">\u003C/video>\n\n## Try It\n```bash\npython3 -m venv .venv && source .venv/bin/activate && pip install -r requirements.txt\ndocker compose up -d\nstreamlit run streamlit_app.py\n```\n","tech_tags":["industrial-ai","procedural-knowledge-extraction","knowledge-graphs","local-llm","privacy-by-design","fastapi","evaluation"],"created_at":"2025-12-24T19:02:47.149096+00:00","updated_at":"2026-02-09T12:18:27.145+00:00","image_url":"https://raw.githubusercontent.com/imaddde867/IPKE/main/assets/banner.jpg","repo_url":"https://github.com/imaddde867/IPKE","demo_url":null,"featured":true},{"id":"1139086b-79f9-44a0-8f81-145dcc137b74","title":"NaviCast: Real-Time Maritime Intelligence Pipeline","description":"Real-time edge-to-cloud pipeline combining AIS ingestion, validated storage, and trajectory forecasting for operational decision support.","full_description":"I built NAVICAST as a challenge from my professor, Tommi Tuomola, who’s passionate about maritime technologies and robust data engineering systems. The idea was simple: stream live AIS vessel data, predict ship trajectories, and serve it all through clean APIs on an interactive map. This project became my hands-on exploration of real-time data pipelines, geospatial visualization, and applied machine learning.\n\n\u003Cp align=\"center\">\n  \u003Cimg src=\"https://raw.githubusercontent.com/imaddde867/NaviCast/main/static/NAVICAST-logo/logo-white.svg\" alt=\"NAVICAST Logo\" width=\"500\"/>\n\u003C/p>\n\n\n## What it does\n\n- Streams live AIS from Digitraffic (Baltic Sea) over MQTT (TLS)\n- Stores normalized snapshots in PostgreSQL with raw JSONB\n- Predicts 30‑minute positions using a trained Random Forest model (with a dead‑reckoning fallback)\n- Exposes a FastAPI layer for vessels, details, downloads, and health\n- Visualizes everything on a Leaflet map with filters, status badges, and prediction overlays\n\n\u003Cp align=\"center\">\n  \u003Cvideo\n    src=\"/dashboard.mp4\"\n    controls\n    preload=\"metadata\"\n    playsinline\n  >\n    Your browser does not support the video tag.\n  \u003C/video>\n\u003C/p>\n\n## How I built it\n\n1) Data ingestion (mqtt_client.py)\n- WebSocket MQTT client with batching (10 messages) and backoff reconnects\n- Field validation and sane defaults for SOG/COG/heading; JSON payloads persisted\n- Primary key on (vessel_id, timestamp) to avoid duplicates; 24‑hour rolling cleanup\n\n2) Storage model (schema.sql)\n- raw_ais_data: latest positions + raw JSONB for flexibility\n- predictions: one row per vessel (updated), plus a unique guard for timestamped predictions\n\n3) Prediction service (prediction_service.py)\n- Loads `vessel_prediction_model.pkl` if present; otherwise uses dead reckoning\n- Runs every 5 minutes and predicts 30 minutes ahead\n- Validates bounds (Baltic window) and outliers; writes upserts; prunes old rows\n\n4) API (api_server.py)\n- Endpoints: `/vessels`, `/vessels/{mmsi}`, `/vessels/download`, `/health`\n- MMSI→country mapping (MID), vessel type/status decoding, optional time filters\n- Returns safe, typed payloads and omits invalid predictions\n\n5) Frontend (static/index.html)\n- Leaflet map with live markers, direction indicators, and predicted paths\n- Filters for All / Moving / Stationary / Predictable (has API prediction)\n- Dark/light mode, compact control panel, CSV/JSON data export\n\n## Model and results\n\nTraining data: 363,899 position records from the Baltic Sea. I compared simple baselines against a Random Forest regressor predicting delta‑lat/delta‑lon for a 30‑minute horizon.\n\nHighlights:\n- Random Forest: mean distance error ≈ 0.154 km, median ≈ 0.011 km\n- Baselines (linear/polynomial, XGBoost) trail in distance error\n- If the model file is missing, the system gracefully falls back to dead reckoning\n\n\u003Cp align=\"center\">\n  \u003Cimg src=\"https://raw.githubusercontent.com/imaddde867/NaviCast/main/static/accuracy.png\" alt=\"Prediction Accuracy Visualization\" width=\"720\"/>\n\u003C/p>\n\n## System architecture\n\n\u003Cp align=\"center\">\n  \u003Cimg src=\"https://raw.githubusercontent.com/imaddde867/NaviCast/main/static/diagram.png\" alt=\"System Architecture Diagram\" width=\"800\"/>\n\u003C/p>\n\nData flow in short:\n- MQTT → Postgres (validated, deduped, JSONB kept)\n- Scheduled predictions → Postgres (upserted, validated)\n- FastAPI → map + exports\n\n## Decisions that mattered\n\n- Keep raw AIS as JSONB: future‑proofs the schema while indexing the fields I care about now.\n- Predict deltas, not absolutes: simpler target space and better generalization across headings.\n- Validate hard at the edges: clip SOG/COG/heading, enforce geographic bounds, skip low‑signal cases.\n- Make failure harmless: if ML fails or is absent, use deterministic dead reckoning so the UI never breaks.\n\n## What was tricky (and how I handled it)\n\n- Noisy AIS fields: normalized ranges and wrote conservative defaults before storage.\n- Duplicate/late data: primary keys + distinct‑on queries in the API.\n- Map performance: minimal DOM work, reuse markers, toggle layers instead of re‑creating.\n- Time windows: sensible defaults on the API (last hour) with explicit ISO overrides.\n\n## Tech stack\n\n- Python, FastAPI, Uvicorn\n- PostgreSQL (JSONB, indexes)\n- Paho‑MQTT (WebSockets + TLS)\n- scikit‑learn, pandas, joblib\n- Leaflet.js (client‑side)\n\n## Run it locally\n\nPrereqs: PostgreSQL 13+, Python 3.9+\n\n```bash\npsql -c \"CREATE DATABASE ais_project;\"\npsql -d ais_project -f schema.sql\npip install -r requirements.txt\n./start_navicast.sh\n# dashboard → http://localhost:8000\n```\n\nTip: set `NAVICAST_DB_*` env vars and `NAVICAST_MODEL_PATH` to point at your DB/model.\n\n## Data and privacy\n\n- Uses public AIS broadcasts; no personal data is collected\n- Raw positions retained ~24h by default; predictions shorter\n- Extensive rotating logs for observability\n\n## What’s next\n\n- Weather/context features into the model\n- Alerting (course deviation, proximity)\n- Historical playback and analytics\n\n—\n\nThis project is a hands‑on snapshot of how I like to build: real‑time first, practical ML, simple interfaces, and resilience by design.\n","tech_tags":["real-time-systems","data-pipelines","time-series","predictive-modeling","mqtt","fastapi","postgresql"],"created_at":"2025-07-06T10:16:11.425063+00:00","updated_at":"2026-02-09T12:18:27.145+00:00","image_url":"https://i.ytimg.com/vi/JgE24u-Taow/maxresdefault.jpg","repo_url":"https://github.com/imaddde867/NaviCast","demo_url":"http://navicast.tech/","featured":true},{"id":"5aae3db6-ac22-487a-9822-bbffc27833e9","title":"Vehicle Detection and Stylization Benchmark (GAN-CNN)","description":"Computer vision benchmark from labeled detection data to deployable models with reproducible training artifacts and model export.","full_description":"This repository documents a four-stage workflow that mixes data engineering, classical model diagnostics, and deployment-oriented training:\n\n1) Dataset curation and annotation in YOLO TXT format (6 vehicle classes).\n2) A custom CNN (TensorFlow/Keras) trained for a high-signal question: \"car vs other\". The notebook shows the diagnosis that the baseline model was dominated by class imbalance, then applies class weighting, Dropout, L2 regularization, and training callbacks to recover minority-class performance.\n3) Multi-class object detection by fine-tuning Ultralytics YOLOv5s with reproducible run logs (`args.yaml`, `results.csv`, plots) and export artifacts (PyTorch weights plus Core ML package for on-device inference).\n4) A lightweight GAN-style stylization stage (\"VividNeonTexture\") that consumes YOLO crops and writes curated, portfolio-ready composites to `results/milestone4/`.\n\nKey quantitative outcomes from committed artifacts:\n- CNN (car vs other): 82.7% -> 91.1% accuracy; car precision 50.8% -> 88.9%; car recall 41.0% -> 56.4%.\n- YOLOv5 (6-class detection): best mAP50-95 reaches 0.90684 with precision 0.95036 and recall 0.94483 (50 epochs, Apple Silicon `device=mps`, AMP, disk cache).\n\n## Pipeline or Architecture\n1. Prepare images + YOLO TXT annotations (class_id, x_center, y_center, width, height)\n2. Build a binary dataset (car = 1, other vehicles = 0) and train a Keras CNN (`best_model.h5`)\n3. Generate `data.yaml` and fine-tune YOLOv5s; track runs under `runs/detect/train_optimized/`\n4. Export the best detector checkpoint and Core ML package (`runs/detect/train_optimized/weights/`)\n5. Run YOLO on images, crop detections, and stylize with VividNeonTexture; save composites to `results/milestone4/`\n\n## Dataset/Training Snapshot\n| Item | Value |\n| --- | --- |\n| Labeled images (YOLO) | 3,000 (`dataset/images/`) |\n| Label files (YOLO) | 3,000 (`dataset/annotations/`) |\n| Vehicle classes | 6 (`car`, `threewheel`, `bus`, `truck`, `motorbike`, `van`) |\n| Total bounding boxes | 3,835 (avg 1.28 boxes/image) |\n| Boxes per class_id | 0: 662, 1: 709, 2: 588, 3: 629, 4: 699, 5: 548 |\n| CNN input / batch | 224x224 RGB, batch 32 |\n| CNN split (binary) | train: 1,718 (cars 345), val: 731 (cars 156) |\n| YOLO config | 50 epochs, batch 16, imgsz 416, lr0 0.01, warmup 3.0, cache=disk, amp=true, device=mps |\n\n## Evaluation/Tracking Snapshot\n| Model | Metric | Value | Source artifact |\n| --- | --- | --- | --- |\n| CNN (baseline) | Accuracy | 82.7% | `main.ipynb` |\n| CNN (baseline) | Car precision / recall | 50.8% / 41.0% | `main.ipynb` |\n| CNN (optimized) | Accuracy | 91.1% | `main.ipynb` |\n| CNN (optimized) | Car precision / recall | 88.9% / 56.4% | `main.ipynb` |\n| YOLOv5 (best epoch) | Precision / recall | 0.95036 / 0.94483 | `runs/detect/train_optimized/results.csv` |\n| YOLOv5 (best epoch) | mAP50 / mAP50-95 | 0.97804 / 0.90684 | `runs/detect/train_optimized/results.csv` |\n| YOLOv5 (training time) | Total time | 8,137.55 s (~2h 15m) | `runs/detect/train_optimized/results.csv` |\n\n## Visual Evidence and Artifacts\n![YOLO predictions on validation batch](https://raw.githubusercontent.com/imaddde867/GAN-CNN/main/runs/detect/train_optimized/val_batch0_pred.jpg)\n\n![YOLO training curves](https://raw.githubusercontent.com/imaddde867/GAN-CNN/main/runs/detect/train_optimized/results.png)\n\n![Normalized confusion matrix](https://raw.githubusercontent.com/imaddde867/GAN-CNN/main/runs/detect/train_optimized/confusion_matrix_normalized.png)\n\n![Precision-Recall curve](https://raw.githubusercontent.com/imaddde867/GAN-CNN/main/runs/detect/train_optimized/BoxPR_curve.png)\n\n![VividNeonTexture sample](https://raw.githubusercontent.com/imaddde867/GAN-CNN/main/results/milestone4/stylized_car366.jpg)\n\n## Engineering Highlights\n- Reproducible experiment tracking: YOLO run metadata (`args.yaml`), per-epoch metrics (`results.csv`), and plots committed under `runs/detect/train_optimized/`\n- Class-imbalance diagnosis and remediation for CNN: explicit baseline vs optimized deltas with class weighting (4.96x), Dropout 0.4, L2=0.01, and callbacks (EarlyStopping, ReduceLROnPlateau)\n- Deployment artifacts included: `best_model.h5` for Keras inference and `runs/detect/train_optimized/weights/best.mlpackage` for Core ML\n- Hardware-aware training configuration: Apple Silicon MPS + AMP + disk cache for faster iterations\n- Modular pipeline reuse: YOLO crops become inputs to the stylization stage, producing curated visuals in `results/milestone4/`\n\n## Try It\n```bash\npython -m venv .venv && source .venv/bin/activate\npip install -U ultralytics tensorflow torch torchvision opencv-python matplotlib jupyter\nyolo detect predict model=runs/detect/train_optimized/weights/best.pt source=dataset/images imgsz=416 save=true\n```\n","tech_tags":["computer-vision","object-detection","model-optimization","yolov5","tensorflow"],"created_at":"2025-12-24T19:03:27.022547+00:00","updated_at":"2026-02-09T12:18:27.145+00:00","image_url":"https://raw.githubusercontent.com/imaddde867/GAN-CNN/main/results/milestone4/stylized_07504.jpg","repo_url":"https://github.com/imaddde867/GAN-CNN","demo_url":null,"featured":false},{"id":"3d321db4-bd68-42d1-beae-f456341eb76e","title":"Spiral Untangler: From-Scratch Neural Network in NumPy","description":"Compact from-scratch neural network experiment focused on nonlinear decision boundaries, training diagnostics, and model behavior inspection.","full_description":"The notebook (`main.ipynb`) generates a synthetic 2-class spiral dataset, normalizes inputs to [-1, 1], and trains a minimalist MLP (2 -> H -> 1) with tanh hidden activations and a sigmoid output. The training loop is fully vectorized and implements binary cross-entropy with a small epsilon for numerical stability plus a from-scratch Adam optimizer. To make model behavior inspectable, it renders decision boundaries on a fine 2D mesh, tracks cost and accuracy, and visualizes uncertainty as |A2 - 0.5|.\n\n## Pipeline or Architecture\n1. Generate intertwined spirals with configurable difficulty/noise and a fixed seed (`generate_intertwined_spirals`).\n2. Normalize inputs by the max absolute value (scale=47.66) and reshape to matrix form.\n3. Initialize parameters with Xavier scaling (`init_params`, seed=42).\n4. Forward pass: `tanh` hidden layer -> `sigmoid` output (`forward_prop`).\n5. Compute binary cross-entropy loss with epsilon (`compute_cost`).\n6. Backpropagate analytic gradients (`back_prop`).\n7. Update weights with Adam (`update_params_adam`).\n8. Plot cost curves and decision boundaries on a fine grid (h=0.02), render a 60-frame boundary-evolution animation, and generate a confidence heatmap.\n\n## Dataset/Training Snapshot\n| Item | Value |\n|---|---|\n| Dataset | Intertwined spirals (2 classes) |\n| Samples | 1000 (500 per class) |\n| Features | 2 (x, y) |\n| Difficulty params | hard, rotations=3.5, noise=0.2 |\n| Normalization | divide by scale=47.66 (max abs of raw X) |\n| Model | 2 -> H -> 1 (tanh, sigmoid) |\n| Optimizer | Adam (lr=0.01, beta1=0.9, beta2=0.999) |\n| Loss | Binary cross-entropy (epsilon=1e-15) |\n| Experiment A (bottleneck) | H=4, 5,000 steps |\n| Experiment B (wide) | H=100, 15,000 steps |\n| Seeds | data=7, weights=42 |\n\n## Evaluation/Tracking Snapshot\n| Epoch (H=100) | Cost (BCE) |\n|---:|---:|\n| 1000 | 0.6007 |\n| 2000 | 0.5173 |\n| 3000 | 0.4505 |\n| 4000 | 0.4149 |\n| 5000 | 0.3656 |\n| 6000 | 0.3065 |\n| 7000 | 0.2490 |\n| 8000 | 0.1930 |\n| 9000 | 0.1457 |\n| 10000 | 0.1082 |\n| 11000 | 0.0860 |\n| 12000 | 0.0711 |\n| 13000 | 0.0598 |\n| 14000 | 0.0499 |\n| 15000 | 0.0418 |\n\n| Setting | Training accuracy |\n|---|---:|\n| Width=4 (underfit) | 59.40% |\n| Width=100 (untangled) | 99.90% |\n\n## Visual Evidence and Artifacts\n![Hard spiral dataset (\"Topological Nightmare\")](https://raw.githubusercontent.com/imaddde867/Spiral-Untagler-ANN/main/results/showcase/dataset_spirals.png)\n\n![Width=4 underfitting (cost curve + boundary)](https://raw.githubusercontent.com/imaddde867/Spiral-Untagler-ANN/main/results/showcase/width4_cost_boundary.png)\n\n![Width=100 boundary after training (cost curve + boundary)](https://raw.githubusercontent.com/imaddde867/Spiral-Untagler-ANN/main/results/showcase/width100_cost_boundary.png)\n\n![Confidence map (where predictions are unsure)](https://raw.githubusercontent.com/imaddde867/Spiral-Untagler-ANN/main/results/showcase/confidence_map.png)\n\n## Engineering Highlights\n- NumPy-only MLP with explicit forward/backprop and vectorized matrix math.\n- Xavier initialization plus a from-scratch Adam optimizer for fast, stable convergence.\n- Deterministic data generation and normalization (fixed seeds and scale factor).\n- Experiment design that isolates model capacity (H=4 vs H=100) with identical training logic.\n- Inspection-first tooling: high-resolution decision boundaries, cost tracking, and an uncertainty heatmap.\n- 60-frame training animation that shows how the boundary evolves over time.\n\n## Tech Stack\nPython, NumPy, Matplotlib, Jupyter Notebook, IPython\n\n## Demo Video\n\u003Cvideo controls src=\"https://raw.githubusercontent.com/imaddde867/Spiral-Untagler-ANN/main/results/showcase/training_boundary_evolution.mp4\" poster=\"https://raw.githubusercontent.com/imaddde867/Spiral-Untagler-ANN/main/results/showcase/width100_cost_boundary.png\">\u003C/video>\n\n## Try It\n1. `python -m venv .venv && source .venv/bin/activate`\n2. `python -m pip install numpy matplotlib jupyter`\n3. `jupyter notebook main.ipynb` (then run all cells)","tech_tags":["ml-fundamentals","from-scratch-ml","numpy","classification"],"created_at":"2025-10-30T05:38:37.266239+00:00","updated_at":"2026-02-09T12:18:27.145+00:00","image_url":"https://raw.githubusercontent.com/imaddde867/Spiral-Untagler-ANN/main/results/showcase/width100_cost_boundary.gif","repo_url":"https://github.com/imaddde867/Spiral-Untagler-ANN","demo_url":null,"featured":false},{"id":"674332e4-f4aa-48ef-9d51-e66ffd0b7f04","title":"InfiniteChessAI (Archived Prototype)","description":"Early exploratory prototype, archived to keep the public portfolio focused on industrial research work.","full_description":"Still working on this one :/\n\nCome back soon, I'll update it as soon as I'm done!","tech_tags":["archived","prototype"],"created_at":"2025-10-30T05:37:33.083863+00:00","updated_at":"2026-02-09T12:18:27.145+00:00","image_url":"https://www.mathsisfun.com/games/images/chess-castle-2.gif","repo_url":"https://github.com/imaddde867/InfiniteChessAI","demo_url":null,"featured":false},{"id":"6dd91268-b428-4803-9100-904bf51b895f","title":"Federated Learning Privacy Benchmark: Gradient Leakage and Defenses","description":"Reproducible benchmark measuring gradient inversion risk and the protection impact of differential privacy and homomorphic encryption.","full_description":"This project is a privacy benchmarking tool for federated learning systems that makes gradient leakage measurable and repeatable. It implements a small CNN training loop across multiple clients, captures victim signals (gradients or one-step updates), and runs a DLG/iDLG-style optimization attack to reconstruct inputs from shared updates. The same pipeline can apply defenses before the attack: (1) Differential Privacy via gradient clipping and Gaussian noise calibrated by epsilon/delta, and (2) an additive homomorphic encryption workflow for protected aggregation (with a fast simulation fallback for large tensors). Each run writes a consistent artifact bundle (reconstruction image, metrics, and config) that is later aggregated into a dashboard and a poster for reporting.\n\nLive dashboard: https://imaddde867.github.io/FL-Attack/\n\n## Pipeline or Architecture\n1. Load CelebA (attribute classification) and preprocess to 64x64 RGB with normalization.\n2. Partition training data IID across clients and run a FedAvg-style round loop.\n3. On the target client/round, capture gradients (or a one-step update) from the first local batch.\n4. Optional defenses:\n   - DP: clip gradients and add Gaussian noise (configured by epsilon, delta, max_norm).\n   - HE: encrypt/quantize updates for aggregation (prototype implementation).\n5. Run gradient inversion (DLG/iDLG-style): optimize a dummy image so its gradients match the captured victim signal (Adam, TV regularization, restarts).\n6. Score reconstructions (MSE, PSNR, SSIM, LPIPS, LabelMatch) and write artifacts to disk.\n7. Aggregate many runs into `results/report/summary.csv` and generate the poster + dashboard assets in `docs/`.\n\n## Dataset/Training Snapshot\nSource: `fl_system.py`, `results/showcase/config.json`, `scripts/run_showcase.sh`.\n\n| Item | Value |\n|---|---|\n| Dataset | CelebA (expected in `data/`; not committed) |\n| Task | Binary attribute classification (`target_attr=\"Male\"`) |\n| Input | 64x64 RGB, normalize mean/std = (0.5, 0.5, 0.5) |\n| Clients | 10 total; 50% sampled per FL round |\n| Subset used (showcase/defenses) | 200 train images, 40 validation images |\n| Global model | SimpleCNN (LeNet-style), 8,760,962 parameters |\n| Client optimizer | SGD, lr=0.01 (momentum varies by run) |\n| FL config (showcase) | 1 round, 1 local epoch, batch_size=1, seed=42 |\n| Attack config (showcase) | Adam lr=0.1, iterations=4500, restarts=5, TV=1e-5, match_metric=l2 |\n\n## Evaluation/Tracking Snapshot\nSource: `results/report/summary.csv`, `results/report/dashboard/data.json`.\n\nDefense benchmarks (single-run metrics):\n\n| Setting | PSNR (dB) | SSIM | LPIPS (lower) | LabelMatch |\n|---|---:|---:|---:|---:|\n| Baseline | 29.38 | 0.920 | 0.117 | 100% |\n| DP (epsilon=8.0) | 6.71 | -0.001 | 0.807 | 0% |\n| DP (epsilon=1.0) | 6.32 | -0.001 | 0.747 | 0% |\n| DP (epsilon=0.1) | 6.36 | -0.001 | 0.806 | 0% |\n| HE | 14.03 | 0.343 | 0.635 | 100% |\n| DP + HE | 6.37 | -0.003 | 0.824 | 0% |\n\nMulti-client baseline variability (10 runs, one per client):\n\n| Metric | Mean | Std | Min | Max |\n|---|---:|---:|---:|---:|\n| PSNR (dB) | 27.29 | 1.21 | 24.91 | 29.51 |\n| SSIM | 0.923 | 0.025 | 0.863 | 0.950 |\n| LPIPS | 0.125 | 0.029 | 0.082 | 0.193 |\n\n## Visual Evidence and Artifacts\n![Architecture diagram (Mermaid-rendered)](https://raw.githubusercontent.com/imaddde867/FL-Attack/main/architecture_mermaid.png)\n\n| Baseline reconstruction example | DP+HE reconstruction example |\n|---|---|\n| ![Baseline reconstruction example](https://raw.githubusercontent.com/imaddde867/FL-Attack/main/docs/assets/images/defenses-baseline-global.png) | ![DP+HE reconstruction example](https://raw.githubusercontent.com/imaddde867/FL-Attack/main/docs/assets/images/defenses-dp-he-global.png) |\n\n![Defense comparison chart](https://raw.githubusercontent.com/imaddde867/FL-Attack/main/docs/assets/charts/defenses_grouped_bars.png)\n\n![Multi-client metric distributions](https://raw.githubusercontent.com/imaddde867/FL-Attack/main/docs/assets/charts/multiclient_boxplots.png)\n\n## Engineering Highlights\n- Reproducible experiment runner with saved configs (`run_experiment.py` writes `config.json` + `metrics.txt` per run).\n- Modular privacy defenses: gradient clipping + DP noise (`differential_privacy.py`) and HE-style encrypted aggregation (`homomorphic_encryptor.py`).\n- Attack pipeline implements optimization-based gradient inversion with restarts, TV regularization, LR scheduling, and optional LPIPS scoring.\n- Standardized artifact outputs (images + metrics) make batch analysis and reporting scriptable (`scripts/analyze_*.py`, `scripts/make_dashboard.py`, `scripts/make_poster*.py`).\n- Dashboard build produces a single source of truth JSON (`results/report/dashboard/data.json`) for plots and run metadata.\n- Device-aware execution (CPU/CUDA/MPS) via `device_utils.py`.\n\n## Try It\nPrereq: download CelebA and place it under `data/` as expected by `fl_system.py`.\n\n```bash\npip install -r requirements.txt\nbash scripts/run_showcase.sh\npython scripts/make_dashboard.py && python -m http.server --directory docs 8000\n```","tech_tags":["privacy-by-design","federated-learning","security-evaluation","gradient-inversion","pytorch"],"created_at":"2025-10-30T05:35:48.326677+00:00","updated_at":"2026-02-09T12:18:27.145+00:00","image_url":"https://raw.githubusercontent.com/imaddde867/FL-Attack/main/docs/assets/images/poster_1080p.png","repo_url":"https://github.com/imaddde867/FL-Attack","demo_url":"https://imaddde867.github.io/FL-Attack/","featured":false},{"id":"80b51708-efa4-4ab8-a97e-9a61ea3bdefc","title":"Term Deposit Subscription Prediction (Tabular ML Benchmark)","description":"Tabular ML pipeline for propensity prediction with feature engineering, leakage-aware modeling, and reproducible evaluation.","full_description":"This project builds a binary classifier on the \"bank-additional-full.csv\" dataset (Bank Marketing with socio/economic context). The pipeline starts from raw CSV ingestion, removes duplicate rows, imputes \"unknown\" categorical values, and encodes mixed feature types (one-hot + cyclic month features). A Random Forest importance pass narrows the input to 19 high-signal features, then a TensorFlow/Keras ANN is tuned and evaluated on a fixed holdout set. The repo ships the full notebook (`ML-Final.ipynb`), a PDF export (`docs/ML-Final.pdf`), and visual diagnostics (`screenshots/`).\n\nDataset reference in repo: `data/bank-additional-names.txt` (Moro et al., 2014, Decision Support Systems).\n\n## Pipeline or Architecture\n1. Ingest raw data from `data/bank-additional-full.csv` (semicolon-delimited).\n2. Data quality pass: drop duplicates; quantify and handle \"unknown\" values in categoricals.\n3. EDA: distribution plots + correlation checks to guide preprocessing and feature pruning.\n4. Feature engineering: one-hot encode categoricals; month encoded with sin/cos cyclic features; scaling for numeric columns.\n5. Leakage control: drop `duration` for modeling (post-call feature noted in dataset description).\n6. Feature selection: Random Forest feature importances -> 19-feature refined set.\n7. Model: ANN (Dense + Dropout + BatchNorm), early stopping + LR scheduling, hyperparameter search.\n8. Evaluation: fixed holdout test set + 5-fold CV accuracy tracking; ROC/AUC + classification report + confusion matrix.\n\n## Dataset/Training Snapshot\n| Item | Value |\n|---|---:|\n| Raw rows / columns | 41,188 / 21 |\n| After de-duplication | 41,176 rows (12 duplicates removed) |\n| Target distribution (deduped) | no: 36,537; yes: 4,639 |\n| Positive rate (yes) | 11.27% |\n| \"unknown\" labels (top columns) | default: 8,596; education: 1,730; housing: 990; loan: 990; job: 330; marital: 80 |\n| Split strategy | 80% train / 10% val / 10% test (two-stage split) |\n| Split sizes (deduped) | train: 32,940; val: 4,118; test: 4,118 |\n| Selected feature count | 19 (post-selection) |\n\n## Evaluation/Tracking Snapshot\n| Metric (holdout test) | Value |\n|---|---:|\n| Accuracy | 0.8961 |\n| ROC AUC | 0.7777 |\n| Class 1 (yes) precision / recall / F1 | 0.76 / 0.16 / 0.27 |\n| Confusion matrix (tn, fp, fn, tp) | 3611, 25, 403, 79 |\n| 5-fold CV accuracy | 0.8992 +/- 0.0022 |\n\n## Visual Evidence and Artifacts\n![Numerical feature distributions](https://raw.githubusercontent.com/imaddde867/Bank-Term-Deposit-Prediction/main/screenshots/numerical_data_analysis.png)\n\n![Categorical feature analysis](https://raw.githubusercontent.com/imaddde867/Bank-Term-Deposit-Prediction/main/screenshots/categorical_data.png)\n\n![Correlation heatmap](https://raw.githubusercontent.com/imaddde867/Bank-Term-Deposit-Prediction/main/screenshots/correlations.png)\n\n![Training curves and initial classification diagnostics](https://raw.githubusercontent.com/imaddde867/Bank-Term-Deposit-Prediction/main/screenshots/initial_classification_results.png)\n\n- Notebook: https://raw.githubusercontent.com/imaddde867/Bank-Term-Deposit-Prediction/main/ML-Final.ipynb\n- PDF report: https://raw.githubusercontent.com/imaddde867/Bank-Term-Deposit-Prediction/main/docs/ML-Final.pdf\n\n## Engineering Highlights\n- Reproducible artifacts: single source-of-truth notebook (`ML-Final.ipynb`) + exported report (`docs/ML-Final.pdf`) + curated plots (`screenshots/`).\n- Data quality controls: explicit duplicate removal and quantified missingness via \"unknown\" labels.\n- Leakage-aware modeling: excludes `duration` (post-call feature) for realistic prediction.\n- Mixed-type feature handling: one-hot categoricals + cyclic time features + scaling for numeric stability.\n- Feature selection: Random Forest importances to reduce dimensionality to 19 inputs.\n- Robust training: early stopping, learning-rate scheduling, and cross-validation tracking for generalization.\n\n## Try It\n```bash\npython -m venv venv && source venv/bin/activate\npip install -r requirements.txt\njupyter notebook ML-Final.ipynb\n```\n","tech_tags":["tabular-ml","classification","feature-engineering","tensorflow","scikit-learn"],"created_at":"2025-07-16T16:51:08.100635+00:00","updated_at":"2026-02-09T12:18:27.145+00:00","image_url":"https://raw.githubusercontent.com/imaddde867/Bank-Term-Deposit-Prediction/main/screenshots/bank-logo-ideas-6-scaled.jpg","repo_url":"https://github.com/imaddde867/Bank-Term-Deposit-Prediction","demo_url":null,"featured":false},{"id":"5cf746e6-934f-400d-b51b-62e0daa4a183","title":"imadlab.me: Research Portfolio Platform","description":"Personal research platform with structured project/blog publishing, Supabase-backed content operations, and analytics instrumentation.","full_description":"## Overview\nThis project is the publishing and operations backbone for my research portfolio.\n\n## Scope\n- Structured project and blog content with Supabase-backed management pages.\n- SEO-aware rendering and prerender scripts for social and search visibility.\n- Practical analytics and newsletter workflows for collaboration outreach.\n\n## Why it matters\nThe platform is designed as a low-friction environment for documenting applied research work with clear technical context, reproducible artifacts, and consistent presentation quality.","tech_tags":["platform-engineering","react","supabase","content-management","analytics"],"created_at":"2025-07-16T15:28:37.47409+00:00","updated_at":"2026-02-09T12:18:27.145+00:00","image_url":"https://raw.githubusercontent.com/imaddde867/imadlab/refs/heads/master/doc/lanyard_imadlab.png","repo_url":"https://github.com/imaddde867/imadlab/","demo_url":"http://imadlab.me/","featured":false},{"id":"51d8de9b-5876-4eda-ac5b-3411814bda84","title":"ClearBox: Secure Messaging Platform Prototype","description":"Secure messaging prototype exploring real-time communication, access control boundaries, and deployable service architecture.","full_description":"## Overview\nClearBox is a full-stack secure messaging prototype built to test real-time communication architecture and security fundamentals.\n\n## System focus\n- Real-time messaging flow and service interaction patterns.\n- Authentication and authorization boundaries.\n- Deployability with containerized services and reverse proxy configuration.\n\n## Engineering outcomes\nThe project served as a practical sandbox for reliability, security posture, and integration discipline in multi-service systems.","tech_tags":["secure-systems","messaging","fastapi","react","postgresql"],"created_at":"2025-07-07T15:59:10.79479+00:00","updated_at":"2026-02-09T12:18:27.145+00:00","image_url":"https://raw.githubusercontent.com/imaddde867/ClearBox/main/clearbox/docs/diagram.png","repo_url":"https://github.com/imaddde867/ClearBox","demo_url":"https://clearbox.live/","featured":false},{"id":"ea66518d-ced2-4d09-b84c-63f57eefd644","title":"Spotify Recommendation Pipeline (Content-Based Clustering)","description":"Recommendation system prototype combining feature engineering, clustering, and retrieval strategies over large audio metadata collections.","full_description":"This project is an advanced music recommendation engine powered by machine learning, designed to deliver highly personalized song suggestions based on audio feature analysis and clustering techniques. Built with a production-ready architecture, it combines a robust backend, a responsive frontend, and sophisticated algorithms to provide a seamless user experience.\n\n## Project Overview\n\nThe system analyzes over 1.1 million Spotify tracks, using Principal Component Analysis (PCA) and K-means clustering to group songs by audio characteristics. It offers two recommendation modes: one driven by Spotify's API for real-time song data and another using a preprocessed dataset for offline capabilities. The result is a scalable, reliable application that delivers tailored music recommendations with sub-second response times.\n\n## Key Features\n\n### Machine Learning Core\n- **Dimensionality Reduction**: PCA implementation preserves 91.99% of data variance, optimizing audio feature processing while reducing noise.\n- **Clustering Algorithm**: K-means groups songs into 35 optimized clusters based on audio similarity patterns.\n- **Feature Engineering**: Analyzes 11 core audio features plus engineered composite metrics like energy-to-acousticness ratios and vocal character analysis.\n- **Similarity Matching**: Uses cosine similarity within clusters for precise song recommendations with comprehensive error handling.\n\n### Web Application\n- **Flask REST API**: Scalable backend with comprehensive error handling and health monitoring.\n- **Responsive Interface**: Modern, accessible frontend with dark mode support and keyboard shortcuts.\n- **Real-Time Processing**: Generates recommendations in under 500ms using pre-trained models.\n- **Multi-level Fallback System**: Ensures uninterrupted service with four distinct recommendation strategies.\n\n### Data Pipeline\n- **Large-scale Dataset**: 1,159,764 Spotify tracks with detailed audio features.\n- **Robust Preprocessing**: Handles missing values, outliers, and feature scaling with validation checks.\n- **Model Persistence**: Optimized model serialization for efficient production deployment.\n\n## Technical Architecture\n\n### System Components\n- **Machine Learning Engine**: Implements PCA, K-means, and similarity calculations (src/recommendation_engine.py).\n- **Web Application**: Flask-based API server and responsive frontend (src/web/app.py and templates).\n- **Data Pipeline**: Feature engineering and model training workflow (notebooks/main.ipynb).\n- **Model Artifacts**: Pre-trained models and preprocessed data (src/models/).\n\n### Audio Features\nThe system processes the following Spotify audio features:\n\n| Feature           | Description                          | Processing         |\n|-------------------|--------------------------------------|--------------------|\n| Danceability      | Suitability for dancing              | Normalized (0-1)   |\n| Energy            | Intensity and activity               | Normalized (0-1)   |\n| Valence           | Musical positivity                   | Normalized (0-1)   |\n| Acousticness      | Acoustic vs. electronic confidence   | Normalized (0-1)   |\n| Instrumentalness  | Likelihood of vocal absence          | Normalized (0-1)   |\n| Liveness          | Presence of live audience            | Normalized (0-1)   |\n| Speechiness       | Presence of spoken words             | Normalized (0-1)   |\n| Tempo             | Speed in BPM                         | MinMax scaled      |\n| Loudness          | Volume in dB                         | Standard scaled    |\n| Key               | Musical key                          | One-hot encoded    |\n| Mode              | Major or minor scale                 | One-hot encoded    |\n\n## Screenshots\n\n### User Interface\n![Application Interface](https://raw.githubusercontent.com/imaddde867/spotify-clusters/main/docs/interface.png)\n\n### Search Functionality\n![Search Interface](https://raw.githubusercontent.com/imaddde867/spotify-clusters/main/docs/search.png)\n\n### Recommendation Results\n![Search Results](https://raw.githubusercontent.com/imaddde867/spotify-clusters/main/docs/result.png)\n\n## How It Works\n\n1. **Input Processing**: User provides a song name and artist, validated and normalized.\n2. **Spotify API Integration**: Fetches audio features for the input song with retry logic and caching.\n3. **Feature Preprocessing**: Scales and transforms features using trained models (StandardScaler, MinMaxScaler).\n4. **Cluster Assignment**: K-means predicts optimal cluster placement for the song.\n5. **Similarity Computation**: Calculates cosine similarity within the assigned cluster for precise matching.\n6. **Result Ranking**: Orders recommendations by similarity with distance-based metrics.\n7. **Fallback Handling**: Implements four levels of fallback strategies when API is unavailable:\n   - **Primary**: Spotify API + ML pipeline\n   - **Secondary**: Dataset fuzzy matching + clustering\n   - **Tertiary**: Random sampling from similar genres\n   - **Emergency**: Popular track recommendations\n\n## Performance Metrics\n\n| Metric                  | Value         | Description                              |\n|-------------------------|---------------|------------------------------------------|\n| Variance Preserved      | 91.99%        | PCA dimensionality reduction efficiency  |\n| Cluster Count           | 35            | Optimized K-means configuration          |\n| Dataset Size            | 1,159,764     | Total tracks analyzed                    |\n| Response Time           | \u003C500ms        | Average recommendation generation        |\n| Memory Usage            | ~2GB          | Model loading and operation requirements |\n\n## Setup Instructions\n\n### Prerequisites\n- Python 3.8+\n- Spotify Developer Account (for API access)\n- 4GB+ RAM (for model loading)\n\n### Steps\n1. **Clone the Repository**:\n   ```bash\n   git clone https://github.com/imaddde867/spotify-clusters.git\n   cd spotify-clusters\n   ```\n2. **Set Up Virtual Environment**:\n   ```bash\n   python -m venv venv\n   source venv/bin/activate  # Windows: venv\\Scripts\\activate\n   ```\n3. **Install Dependencies**:\n   ```bash\n   pip install -r requirements.txt\n   ```\n4. **Configure Spotify API**:\n   - Copy .env.example to .env\n   - Add your Spotify Client ID and Client Secret to .env\n\n### Running the Application\n- **Development**:\n   ```bash\n   python src/web/app.py\n   # Or use convenience script\n   ./run.sh  # Unix/Mac\n   run.bat   # Windows\n   ```\n- **Production**:\n   ```bash\n   gunicorn --bind 0.0.0.0:5000 --workers 4 src.web.app:app\n   ```\n\nAccess the app at `http://localhost:5001` (development) or `http://localhost:5000` (production).\n\n## API Documentation\n\n- **GET /**: Serves the main application interface.\n- **POST /recommend**: Generates music recommendations.\n  - Example Request:\n    ```json\n    {\n      \"song_name\": \"Bohemian Rhapsody\",\n      \"artist_name\": \"Queen\",\n      \"playlist_size\": 10\n    }\n    ```\n  - Response includes track names, artists, genres, and popularity scores.\n- **GET /api/popular-examples**: Provides sample songs for the interface.\n- **GET /health**: System health check endpoint with component status.\n\n## Development Insights\n\n### Technical Challenges Addressed\n- **Memory Optimization**: Implemented efficient data loading and processing for the 1.1M+ track dataset.\n- **API Resilience**: Created a robust caching system to reduce Spotify API calls and handle rate limits.\n- **Error Recovery**: Designed comprehensive try/except blocks with multiple fallback mechanisms.\n- **Frontend Accessibility**: Implemented keyboard shortcuts, screen reader support, and responsive design.\n\n### Future Enhancements\n- Integrate a database backend for faster data retrieval and user preferences.\n- Add user playlist analysis for personalized clustering and recommendations.\n- Enhance frontend with interactive audio feature visualizations.\n- Implement collaborative filtering to complement content-based recommendations.\n\n## Technologies Used\n- **Backend**: Python 3.8+, Flask 2.0+, scikit-learn, pandas, numpy, spotipy\n- **Frontend**: HTML5, CSS3, Vanilla JavaScript with accessibility features\n- **Deployment**: Gunicorn-ready, Docker-compatible\n- **Data**: Spotify API, 1.1M+ track dataset with 11 core audio features\n\n## Project Status\n- **Status**: Production-Ready\n- **Maintenance**: Active Development\n- **Performance**: Sub-second recommendation generation\n\nThis project showcases my expertise in machine learning, web development, and API integration, delivering a practical and engaging solution for music discovery that combines sophisticated algorithms with a polished user experience.","tech_tags":["recommendation-systems","ml-pipelines","flask","feature-engineering","clustering"],"created_at":"2025-07-06T10:15:07.421174+00:00","updated_at":"2026-02-09T12:18:27.145+00:00","image_url":"https://substackcdn.com/image/fetch/$s_!X0-a!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb84e92f0-cb22-458e-aa27-9abb9a602fdd_1421x699.png","repo_url":"https://github.com/imaddde867/spotify-clusters","demo_url":null,"featured":false}];</script>
</body>
</html>
