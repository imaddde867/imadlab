<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <!-- Meta tags dynamically injected by <Seo /> component via react-helmet-async -->
    <!-- Static fallbacks only for crawlers that don't execute JS -->
    <title>Federated Learning Privacy Benchmark: Gradient Leakage and Defenses | Imadlab | Research Engineer &amp; Internal CTO</title>
    <meta name="description" content="Reproducible benchmark measuring gradient inversion risk and the protection impact of differential privacy and homomorphic encryption." />
    <!-- prerender-seo:start -->
    <link rel="canonical" href="https://imadlab.me/projects/6dd91268-b428-4803-9100-904bf51b895f" />
    <meta property="og:title" content="Federated Learning Privacy Benchmark: Gradient Leakage and Defenses | Imadlab | Research Engineer &amp; Internal CTO" />
    <meta property="og:description" content="Reproducible benchmark measuring gradient inversion risk and the protection impact of differential privacy and homomorphic encryption." />
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://imadlab.me/projects/6dd91268-b428-4803-9100-904bf51b895f" />
    <meta property="og:image" content="https://raw.githubusercontent.com/imaddde867/FL-Attack/main/docs/assets/images/poster_1080p.png" />
    <meta property="og:image:alt" content="Federated Learning Privacy Benchmark: Gradient Leakage and Defenses" />
    <meta property="og:locale" content="en_US" />
    <meta property="og:site_name" content="Imadlab" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:site" content="@imadlab" />
    <meta name="twitter:creator" content="@imadlab" />
    <meta name="twitter:title" content="Federated Learning Privacy Benchmark: Gradient Leakage and Defenses | Imadlab | Research Engineer &amp; Internal CTO" />
    <meta name="twitter:description" content="Reproducible benchmark measuring gradient inversion risk and the protection impact of differential privacy and homomorphic encryption." />
    <meta name="twitter:image" content="https://raw.githubusercontent.com/imaddde867/FL-Attack/main/docs/assets/images/poster_1080p.png" />
    <meta name="twitter:image:alt" content="Federated Learning Privacy Benchmark: Gradient Leakage and Defenses" />
    <script type="application/ld+json">{"@context":"https://schema.org","@type":"SoftwareApplication","name":"Federated Learning Privacy Benchmark: Gradient Leakage and Defenses","description":"Reproducible benchmark measuring gradient inversion risk and the protection impact of differential privacy and homomorphic encryption.","url":"https://imadlab.me/projects/6dd91268-b428-4803-9100-904bf51b895f","image":"https://raw.githubusercontent.com/imaddde867/FL-Attack/main/docs/assets/images/poster_1080p.png","codeRepository":"https://github.com/imaddde867/FL-Attack","sameAs":["https://github.com/imaddde867/FL-Attack","https://imaddde867.github.io/FL-Attack/"],"datePublished":"2025-10-30T05:35:48.326677+00:00","dateModified":"2026-02-09T12:18:27.145+00:00","author":{"@type":"Person","name":"Imad Eddine El Mouss","url":"https://imadlab.me/about"}}</script>
    <!-- prerender-seo:end -->

    <!-- Favicon and Theme Color -->
    <link rel="icon" href="/favicon.ico" sizes="any">
    <link rel="apple-touch-icon" href="/apple-touch-icon.png">
    <meta name="theme-color" content="#000000" />

    <!-- Preload hero image for faster LCP on home route -->
    <link rel="preload" as="image" href="/images/hero-moon.avif" type="image/avif" />

    <!-- Font Preconnect for Performance -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    
    <!-- Load fonts with font-display: swap for better performance -->
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800;900&family=JetBrains+Mono:wght@300;400;500;600;700&display=swap" rel="stylesheet" media="print" onload="this.media='all'">
    <noscript>
      <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800;900&family=JetBrains+Mono:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    </noscript>
    
    <!-- DNS prefetch for Supabase and Cloudflare Analytics -->
    <link rel="dns-prefetch" href="https://mpkgugcasxpanhrkpkhs.supabase.co">
    <link rel="preconnect" href="https://mpkgugcasxpanhrkpkhs.supabase.co" crossorigin>
    <link rel="dns-prefetch" href="https://static.cloudflareinsights.com">
    <link rel="preconnect" href="https://static.cloudflareinsights.com" crossorigin>
    
    <!-- RSS/JSON Feeds -->
    <link rel="alternate" type="application/rss+xml" title="Imadlab Blog RSS Feed" href="/feed.xml" />
    <link rel="alternate" type="application/feed+json" title="Imadlab Blog JSON Feed" href="/feed.json" />
    <meta name="referrer" content="strict-origin-when-cross-origin" />

    <!-- Structured Data -->
    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "WebSite",
      "url": "https://imadlab.me",
      "name": "Imadlab",
      "description": "Applied research and engineering work by Imad Eddine El Mouss on multimodal industrial AI, procedural knowledge extraction, and deployable systems.",
      "publisher": {
        "@type": "Person",
        "name": "Imad Eddine El Mouss",
        "url": "https://imadlab.me/about",
        "sameAs": [
          "https://github.com/imaddde867",
          "https://www.linkedin.com/in/imad-eddine-e-986741262"
        ]
      },
      "potentialAction": {
        "@type": "SearchAction",
        "target": "https://duckduckgo.com/?q=site%3Aimadlab.me+{search_term_string}",
        "query-input": "required name=search_term_string"
      }
    }
    </script>
    <script type="module" crossorigin src="/assets/js/index-B1IGmRtD.js"></script>
    <link rel="stylesheet" crossorigin href="/assets/index-9eB0Q-OK.css">
  </head>

  <body>
    <h1 style="position: absolute; width: 1px; height: 1px; padding: 0; margin: -1px; overflow: hidden; clip: rect(0, 0, 0, 0); border: 0;">Imadlab | Research Engineer & Internal CTO</h1>
    <div id="root">
<main data-prerender="true" class="prerender-shell">
  <article class="prerender-article">
    <p class="prerender-meta text-sm text-white/60">October 30, 2025</p>
    <h1 class="text-3xl font-bold mb-4">Federated Learning Privacy Benchmark: Gradient Leakage and Defenses</h1>
    <p class="prerender-tags">Tech: privacy-by-design, federated-learning, security-evaluation, gradient-inversion, pytorch</p>
    <div class="prerender-image"><img src="https://raw.githubusercontent.com/imaddde867/FL-Attack/main/docs/assets/images/poster_1080p.png" alt="Federated Learning Privacy Benchmark: Gradient Leakage and Defenses" loading="lazy" decoding="async" /></div>
    <p class="prerender-summary leading-relaxed">This project is a privacy benchmarking tool for federated learning systems that makes gradient leakage measurable and repeatable. It implements a small CNN training loop across multiple clients, captures victim signals (gradients or one-step updates), and runs a DLG/iDLG-style optimization attack to reconstruct inputs from shared updates. The same pipeline can apply defenses before the attack: (1) Differential Pri...</p>
    <p class="prerender-meta mt-4">Source: <a class="prerender-link inline" href="https://github.com/imaddde867/FL-Attack" rel="noopener">View repository</a></p>
    <a class="prerender-link mt-6 inline-flex" href="/projects/6dd91268-b428-4803-9100-904bf51b895f">Continue exploring</a>
  </article>
</main>
</div>
    
  <script>window.__PRERENDERED_DATA__ = window.__PRERENDERED_DATA__ || {}; window.__PRERENDERED_DATA__["project:6dd91268-b428-4803-9100-904bf51b895f"] = {"id":"6dd91268-b428-4803-9100-904bf51b895f","title":"Federated Learning Privacy Benchmark: Gradient Leakage and Defenses","description":"Reproducible benchmark measuring gradient inversion risk and the protection impact of differential privacy and homomorphic encryption.","full_description":"This project is a privacy benchmarking tool for federated learning systems that makes gradient leakage measurable and repeatable. It implements a small CNN training loop across multiple clients, captures victim signals (gradients or one-step updates), and runs a DLG/iDLG-style optimization attack to reconstruct inputs from shared updates. The same pipeline can apply defenses before the attack: (1) Differential Privacy via gradient clipping and Gaussian noise calibrated by epsilon/delta, and (2) an additive homomorphic encryption workflow for protected aggregation (with a fast simulation fallback for large tensors). Each run writes a consistent artifact bundle (reconstruction image, metrics, and config) that is later aggregated into a dashboard and a poster for reporting.\n\nLive dashboard: https://imaddde867.github.io/FL-Attack/\n\n## Pipeline or Architecture\n1. Load CelebA (attribute classification) and preprocess to 64x64 RGB with normalization.\n2. Partition training data IID across clients and run a FedAvg-style round loop.\n3. On the target client/round, capture gradients (or a one-step update) from the first local batch.\n4. Optional defenses:\n   - DP: clip gradients and add Gaussian noise (configured by epsilon, delta, max_norm).\n   - HE: encrypt/quantize updates for aggregation (prototype implementation).\n5. Run gradient inversion (DLG/iDLG-style): optimize a dummy image so its gradients match the captured victim signal (Adam, TV regularization, restarts).\n6. Score reconstructions (MSE, PSNR, SSIM, LPIPS, LabelMatch) and write artifacts to disk.\n7. Aggregate many runs into `results/report/summary.csv` and generate the poster + dashboard assets in `docs/`.\n\n## Dataset/Training Snapshot\nSource: `fl_system.py`, `results/showcase/config.json`, `scripts/run_showcase.sh`.\n\n| Item | Value |\n|---|---|\n| Dataset | CelebA (expected in `data/`; not committed) |\n| Task | Binary attribute classification (`target_attr=\"Male\"`) |\n| Input | 64x64 RGB, normalize mean/std = (0.5, 0.5, 0.5) |\n| Clients | 10 total; 50% sampled per FL round |\n| Subset used (showcase/defenses) | 200 train images, 40 validation images |\n| Global model | SimpleCNN (LeNet-style), 8,760,962 parameters |\n| Client optimizer | SGD, lr=0.01 (momentum varies by run) |\n| FL config (showcase) | 1 round, 1 local epoch, batch_size=1, seed=42 |\n| Attack config (showcase) | Adam lr=0.1, iterations=4500, restarts=5, TV=1e-5, match_metric=l2 |\n\n## Evaluation/Tracking Snapshot\nSource: `results/report/summary.csv`, `results/report/dashboard/data.json`.\n\nDefense benchmarks (single-run metrics):\n\n| Setting | PSNR (dB) | SSIM | LPIPS (lower) | LabelMatch |\n|---|---:|---:|---:|---:|\n| Baseline | 29.38 | 0.920 | 0.117 | 100% |\n| DP (epsilon=8.0) | 6.71 | -0.001 | 0.807 | 0% |\n| DP (epsilon=1.0) | 6.32 | -0.001 | 0.747 | 0% |\n| DP (epsilon=0.1) | 6.36 | -0.001 | 0.806 | 0% |\n| HE | 14.03 | 0.343 | 0.635 | 100% |\n| DP + HE | 6.37 | -0.003 | 0.824 | 0% |\n\nMulti-client baseline variability (10 runs, one per client):\n\n| Metric | Mean | Std | Min | Max |\n|---|---:|---:|---:|---:|\n| PSNR (dB) | 27.29 | 1.21 | 24.91 | 29.51 |\n| SSIM | 0.923 | 0.025 | 0.863 | 0.950 |\n| LPIPS | 0.125 | 0.029 | 0.082 | 0.193 |\n\n## Visual Evidence and Artifacts\n![Architecture diagram (Mermaid-rendered)](https://raw.githubusercontent.com/imaddde867/FL-Attack/main/architecture_mermaid.png)\n\n| Baseline reconstruction example | DP+HE reconstruction example |\n|---|---|\n| ![Baseline reconstruction example](https://raw.githubusercontent.com/imaddde867/FL-Attack/main/docs/assets/images/defenses-baseline-global.png) | ![DP+HE reconstruction example](https://raw.githubusercontent.com/imaddde867/FL-Attack/main/docs/assets/images/defenses-dp-he-global.png) |\n\n![Defense comparison chart](https://raw.githubusercontent.com/imaddde867/FL-Attack/main/docs/assets/charts/defenses_grouped_bars.png)\n\n![Multi-client metric distributions](https://raw.githubusercontent.com/imaddde867/FL-Attack/main/docs/assets/charts/multiclient_boxplots.png)\n\n## Engineering Highlights\n- Reproducible experiment runner with saved configs (`run_experiment.py` writes `config.json` + `metrics.txt` per run).\n- Modular privacy defenses: gradient clipping + DP noise (`differential_privacy.py`) and HE-style encrypted aggregation (`homomorphic_encryptor.py`).\n- Attack pipeline implements optimization-based gradient inversion with restarts, TV regularization, LR scheduling, and optional LPIPS scoring.\n- Standardized artifact outputs (images + metrics) make batch analysis and reporting scriptable (`scripts/analyze_*.py`, `scripts/make_dashboard.py`, `scripts/make_poster*.py`).\n- Dashboard build produces a single source of truth JSON (`results/report/dashboard/data.json`) for plots and run metadata.\n- Device-aware execution (CPU/CUDA/MPS) via `device_utils.py`.\n\n## Try It\nPrereq: download CelebA and place it under `data/` as expected by `fl_system.py`.\n\n```bash\npip install -r requirements.txt\nbash scripts/run_showcase.sh\npython scripts/make_dashboard.py && python -m http.server --directory docs 8000\n```","tech_tags":["privacy-by-design","federated-learning","security-evaluation","gradient-inversion","pytorch"],"created_at":"2025-10-30T05:35:48.326677+00:00","updated_at":"2026-02-09T12:18:27.145+00:00","image_url":"https://raw.githubusercontent.com/imaddde867/FL-Attack/main/docs/assets/images/poster_1080p.png","repo_url":"https://github.com/imaddde867/FL-Attack","demo_url":"https://imaddde867.github.io/FL-Attack/","featured":false};</script>
</body>
</html>
