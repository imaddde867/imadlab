<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <!-- Meta tags dynamically injected by <Seo /> component via react-helmet-async -->
    <!-- Static fallbacks only for crawlers that don't execute JS -->
    <title>Multi-View Industrial Context Tracking | Imadlab | Research Engineer &amp; Internal CTO</title>
    <meta name="description" content="End-to-end multi-camera perception prototype with auto-labeling, detector training, and cross-camera identity consistency for operational context awareness." />
    <!-- prerender-seo:start -->
    <link rel="canonical" href="https://imadlab.me/projects/bb3b627c-3e6e-46e1-ada8-a0764e991a66" />
    <meta property="og:title" content="Multi-View Industrial Context Tracking | Imadlab | Research Engineer &amp; Internal CTO" />
    <meta property="og:description" content="End-to-end multi-camera perception prototype with auto-labeling, detector training, and cross-camera identity consistency for operational context awareness." />
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://imadlab.me/projects/bb3b627c-3e6e-46e1-ada8-a0764e991a66" />
    <meta property="og:image" content="https://imadlab.me/images/projects/multi-view-industrial-context-tracking.gif" />
    <meta property="og:image:alt" content="Multi-View Industrial Context Tracking" />
    <meta property="og:locale" content="en_US" />
    <meta property="og:site_name" content="Imadlab" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:site" content="@imadlab" />
    <meta name="twitter:creator" content="@imadlab" />
    <meta name="twitter:title" content="Multi-View Industrial Context Tracking | Imadlab | Research Engineer &amp; Internal CTO" />
    <meta name="twitter:description" content="End-to-end multi-camera perception prototype with auto-labeling, detector training, and cross-camera identity consistency for operational context awareness." />
    <meta name="twitter:image" content="https://imadlab.me/images/projects/multi-view-industrial-context-tracking.gif" />
    <meta name="twitter:image:alt" content="Multi-View Industrial Context Tracking" />
    <script type="application/ld+json">{"@context":"https://schema.org","@type":"SoftwareApplication","name":"Multi-View Industrial Context Tracking","description":"End-to-end multi-camera perception prototype with auto-labeling, detector training, and cross-camera identity consistency for operational context awareness.","url":"https://imadlab.me/projects/bb3b627c-3e6e-46e1-ada8-a0764e991a66","image":"https://imadlab.me/images/projects/multi-view-industrial-context-tracking.gif","codeRepository":"https://github.com/imaddde867/Multi-view-object-detection","sameAs":["https://github.com/imaddde867/Multi-view-object-detection"],"datePublished":"2025-12-24T19:03:27.022547+00:00","dateModified":"2026-02-09T12:18:27.145+00:00","author":{"@type":"Person","name":"Imad Eddine El Mouss","url":"https://imadlab.me/about"}}</script>
    <!-- prerender-seo:end -->

    <!-- Favicon and Theme Color -->
    <link rel="icon" href="/favicon.ico" sizes="any">
    <link rel="apple-touch-icon" href="/apple-touch-icon.png">
    <meta name="theme-color" content="#000000" />

    <!-- Preload hero image for faster LCP on home route -->
    <link rel="preload" as="image" href="/images/hero-moon.avif" type="image/avif" />

    <!-- Font Preconnect for Performance -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    
    <!-- Load fonts with font-display: swap for better performance -->
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800;900&family=JetBrains+Mono:wght@300;400;500;600;700&display=swap" rel="stylesheet" media="print" onload="this.media='all'">
    <noscript>
      <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800;900&family=JetBrains+Mono:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    </noscript>
    
    <!-- DNS prefetch for Supabase and Cloudflare Analytics -->
    <link rel="dns-prefetch" href="https://mpkgugcasxpanhrkpkhs.supabase.co">
    <link rel="preconnect" href="https://mpkgugcasxpanhrkpkhs.supabase.co" crossorigin>
    <link rel="dns-prefetch" href="https://static.cloudflareinsights.com">
    <link rel="preconnect" href="https://static.cloudflareinsights.com" crossorigin>
    
    <!-- RSS/JSON Feeds -->
    <link rel="alternate" type="application/rss+xml" title="Imadlab Blog RSS Feed" href="/feed.xml" />
    <link rel="alternate" type="application/feed+json" title="Imadlab Blog JSON Feed" href="/feed.json" />
    <meta name="referrer" content="strict-origin-when-cross-origin" />

    <!-- Structured Data -->
    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "WebSite",
      "url": "https://imadlab.me",
      "name": "Imadlab",
      "description": "Applied research and engineering work by Imad Eddine El Mouss on multimodal industrial AI, procedural knowledge extraction, and deployable systems.",
      "publisher": {
        "@type": "Person",
        "name": "Imad Eddine El Mouss",
        "url": "https://imadlab.me/about",
        "sameAs": [
          "https://github.com/imaddde867",
          "https://www.linkedin.com/in/imad-eddine-e-986741262"
        ]
      },
      "potentialAction": {
        "@type": "SearchAction",
        "target": "https://duckduckgo.com/?q=site%3Aimadlab.me+{search_term_string}",
        "query-input": "required name=search_term_string"
      }
    }
    </script>
    <script type="module" crossorigin src="/assets/js/index-B1IGmRtD.js"></script>
    <link rel="stylesheet" crossorigin href="/assets/index-9eB0Q-OK.css">
  </head>

  <body>
    <h1 style="position: absolute; width: 1px; height: 1px; padding: 0; margin: -1px; overflow: hidden; clip: rect(0, 0, 0, 0); border: 0;">Imadlab | Research Engineer & Internal CTO</h1>
    <div id="root">
<main data-prerender="true" class="prerender-shell">
  <article class="prerender-article">
    <p class="prerender-meta text-sm text-white/60">December 24, 2025</p>
    <h1 class="text-3xl font-bold mb-4">Multi-View Industrial Context Tracking</h1>
    <p class="prerender-tags">Tech: industrial-ai, computer-vision, multimodal-context, cross-camera-tracking, pytorch, mlflow</p>
    <div class="prerender-image"><img src="https://imadlab.me/images/projects/multi-view-industrial-context-tracking.gif" alt="Multi-View Industrial Context Tracking" loading="lazy" decoding="async" /></div>
    <p class="prerender-summary leading-relaxed">This project is my full-stack take on multi-view perception: data engineering for large-scale labeling, model training, and robust cross-camera tracking. The pipeline is config-driven and reproducible end-to-end using a CLI workflow (`multiview label | train | run`).

**Pipeline**
1. **Labeling**: Sample synchronized frames across camera groups, propose boxes with YOLO, then refine masks with SAM3.
2. **Training**...</p>
    <p class="prerender-meta mt-4">Source: <a class="prerender-link inline" href="https://github.com/imaddde867/Multi-view-object-detection" rel="noopener">View repository</a></p>
    <a class="prerender-link mt-6 inline-flex" href="/projects/bb3b627c-3e6e-46e1-ada8-a0764e991a66">Continue exploring</a>
  </article>
</main>
</div>
    
  <script>window.__PRERENDERED_DATA__ = window.__PRERENDERED_DATA__ || {}; window.__PRERENDERED_DATA__["project:bb3b627c-3e6e-46e1-ada8-a0764e991a66"] = {"id":"bb3b627c-3e6e-46e1-ada8-a0764e991a66","title":"Multi-View Industrial Context Tracking","description":"End-to-end multi-camera perception prototype with auto-labeling, detector training, and cross-camera identity consistency for operational context awareness.","full_description":"This project is my full-stack take on multi-view perception: data engineering for large-scale labeling, model training, and robust cross-camera tracking. The pipeline is config-driven and reproducible end-to-end using a CLI workflow (`multiview label | train | run`).\n\n**Pipeline**\n1. **Labeling**: Sample synchronized frames across camera groups, propose boxes with YOLO, then refine masks with SAM3.\n2. **Training**: Train a YOLO detector on the generated dataset.\n3. **System run**: Detect and track per camera, then associate tracks across cameras to assign global IDs.\n\n**Dataset and Training Snapshot**\n| Item | Value |\n| --- | --- |\n| Source videos | 7 synchronized cameras |\n| Images labeled | 3,104 |\n| Total labels | 34,198 |\n| Labels per image | 11.0 |\n| Classes | person, car, bus |\n| Frame stride | 5 |\n| Base model | YOLO11m |\n| Train config | 100 epochs, 960 imgsz, batch 16 |\n\n**Multi-View Tracking Snapshot (Group g34)**\n| Item | Value |\n| --- | --- |\n| Frames | 1,757 |\n| Cameras | cam3 + cam4 |\n| Total detections | 38,253 |\n| Avg detections per frame | 21.8 |\n| Global IDs | 161 (70 person, 91 car) |\n| Class counts | 9,468 person, 28,785 car |\n\n**Visual Evidence and Artifacts**\n![SAM3 auto-labeling visualization](https://raw.githubusercontent.com/imaddde867/Multi-view-object-detection/main/data/processed/showcase/sam3_autolabel_v2/viz/train/Cam3_f000025.jpg)\n*SAM3-refined labels over YOLO proposals from the auto-labeling run.*\n\n![Training batch preview](https://raw.githubusercontent.com/imaddde867/Multi-view-object-detection/main/results/showcase/training/sam3_autolabel_v2/train_batch0.jpg)\n*Training samples from the YOLO pipeline.*\n\n![Validation predictions](https://raw.githubusercontent.com/imaddde867/Multi-view-object-detection/main/results/showcase/training/sam3_autolabel_v2/val_batch0_pred.jpg)\n*Validation predictions from the trained model.*\n\n![Confusion matrix](https://raw.githubusercontent.com/imaddde867/Multi-view-object-detection/main/results/showcase/training/sam3_autolabel_v2/confusion_matrix_normalized.png)\n*Normalized confusion matrix for the trained detector.*\n\n![Training curves](https://raw.githubusercontent.com/imaddde867/Multi-view-object-detection/main/results/showcase/training/sam3_autolabel_v2/results.png)\n*Training curves from the YOLO run.*\n\n![Precision/Recall trend](https://raw.githubusercontent.com/imaddde867/Multi-view-object-detection/main/results/showcase/training/sam3_autolabel_v2/BoxF1_curve.png)\n*F1 score trend across epochs.*\n\nDemo video:\n\u003Cvideo controls src=\"https://raw.githubusercontent.com/imaddde867/Multi-view-object-detection/main/results/showcase/system/sam3_autolabel_v2/g34_demo.mp4\" poster=\"https://raw.githubusercontent.com/imaddde867/Multi-view-object-detection/main/results/showcase/training/sam3_autolabel_v2/val_batch0_pred.jpg\">\u003C/video>\n\n**Engineering Highlights**\n- **Reproducibility**: Config-driven runs, tracked artifacts under `results/showcase/`.\n- **Scalable labeling**: SAM3 refinement generates training data quickly without manual annotation.\n- **Cross-camera consistency**: Global ID assignment keeps identities stable across camera views.\n- **Deployment-ready**: CLI pipeline supports CPU/GPU toggles and SLURM job scripts for HPC runs.\n\n**Try it**\n```bash\nmultiview label --config config/labeling.yaml\nmultiview train --config config/train.yaml\nmultiview run --config config/system_demo_tuned.yaml\n```","tech_tags":["industrial-ai","computer-vision","multimodal-context","cross-camera-tracking","pytorch","mlflow"],"created_at":"2025-12-24T19:03:27.022547+00:00","updated_at":"2026-02-09T12:18:27.145+00:00","image_url":"https://imadlab.me/images/projects/multi-view-industrial-context-tracking.gif","repo_url":"https://github.com/imaddde867/Multi-view-object-detection","demo_url":null,"featured":true};</script>
</body>
</html>
